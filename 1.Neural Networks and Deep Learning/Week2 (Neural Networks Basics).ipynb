{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b948e29",
   "metadata": {},
   "source": [
    "# 1.Logistic Regression as a Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cef1a4",
   "metadata": {},
   "source": [
    "### Banary Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab67387",
   "metadata": {},
   "source": [
    "뉴럴 네트워크 프로그래밍의 기본\n",
    "\n",
    "뉴럴 네트워크를 구현할 때 특정 기술이 중요하다. 예를 들면, m개의 훈련 예제가 있을 때, 일반적으로 for 루프를 사용하여 m개의 훈련 예제를 처리하게 될 것이다. 그러나 뉴럴 네트워크를 구현할 때는 전체 훈련 세트를 for 루프 없이 처리하려고 한다. 또한 뉴럴 네트워크의 계산을 조직할 때 전파 단계와 역전파 단계가 있다. 이번 주 자료에서는 이러한 개념을 로지스틱 회귀를 사용하여 소개한다.\n",
    "\n",
    "로지스틱 회귀는 이진 분류 알고리즘이다. 예를 들어 이미지가 고양이인지 아닌지를 구분하는 문제가 있다면, 출력 라벨 y로 이미지가 고양이(1)인지 아닌지(0)를 나타낼 수 있다. 컴퓨터에서 이미지는 빨강, 초록, 파랑의 세 가지 색상 채널에 해당하는 행렬로 저장된다. 이러한 픽셀 값을 기능 벡터 x로 변환하기 위해 모든 픽셀 값을 언롤링한다.\n",
    "\n",
    "이진 분류에서는 이러한 이미지를 입력으로 받아 y 라벨이 1인지 0인지 예측하는 분류기를 학습하는 것이 목표다. 각 훈련 예제는 (x,y) 쌍으로 표현되며, x는 n차원의 특성 벡터이고 y는 0 또는 1이다. 전체 훈련 세트는 m개의 훈련 예제로 구성되며, 이를 행렬 형태로 나타낼 수 있다.\n",
    "\n",
    "이렇게 정의된 표기법을 사용하면 로지스틱 회귀와 뉴럴 네트워크를 효과적으로 구현할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10157ec",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7513f7c",
   "metadata": {},
   "source": [
    "로지스틱 회귀(logistic regression)는 지도 학습 문제에서 출력 레이블 Y가 모두 0 또는 1인 경우 사용하는 학습 알고리즘이다. 이것은 이진 분류 문제에 적합하다. 예를 들어, 고양이 사진인지 아닌지를 판별하려는 이미지와 같은 입력 특성 벡터 X가 주어지면, 로지스틱 회귀는 Y의 추정값인 Y hat을 출력한다. 우리는 Y hat이 X 주어진 상태에서 Y가 1일 확률을 나타내기를 원한다.\n",
    "\n",
    "로지스틱 회귀의 파라미터는 W(차원의 벡터)와 b(실수)로 구성된다. 단순히 입력 X에 대한 선형 함수로 Y hat을 생성하는 것은 좋은 방법이 아니다. 이는 Y hat이 0과 1 사이의 값, 즉 확률로 표현되어야 하기 때문이다. 따라서 로지스틱 회귀에서는 출력값으로 시그모이드 함수를 사용하여 Y hat을 생성한다.\n",
    "\n",
    "시그모이드 함수는 Z 값에 따라 0에서 1 사이의 값을 가진다. 이 함수의 수식은 σ(Z)=1+e−Z1 이다. Z가 매우 큰 값이면 시그모이드 함수는 1에 가깝게 되고, Z가 매우 작은 값이면 0에 가깝게 된다.\n",
    "\n",
    "로지스틱 회귀를 구현할 때 목표는 파라미터 W와 B를 학습하여 Y hat이 Y=1일 확률의 좋은 추정값이 되게 하는 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd50a9bd",
   "metadata": {},
   "source": [
    "### Logistic Regression Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e033dc4d",
   "metadata": {},
   "source": [
    "모델 출력:\n",
    "\n",
    "로지스틱 회귀의 출력인 y_hat은 입력 X의 선형 결합에 적용된 시그모이드 함수로 주어집니다. 수식은 다음과 같습니다: y_hat = sigmoid(W^T * X + b). 여기서 W는 가중치, X는 입력, b는 편향입니다.\n",
    "\n",
    "<br> 손실 함수:\n",
    "\n",
    "MSE(Mean Squared Error)를 사용하는 것이 간단한 방법일 수 있지만 로지스틱 회귀에서는 최적화 문제가 있어 선호되지 않습니다.\n",
    "대신 로지스틱 회귀의 손실 함수는 다음과 같이 정의됩니다:\n",
    "L(y_hat, y) = -[y * log(y_hat) + (1-y) * log(1-y_hat)].\n",
    "\n",
    "실제 y가 1일 경우, 손실 함수는 y_hat을 최대한 크게 만들려고 하며, 이는 1에 가깝게 만들려는 것입니다.\n",
    "실제 y가 0일 경우, 손실 함수는 y_hat을 최대한 작게 만들려고 하며, 이는 0에 가깝게 만들려는 것입니다.\n",
    "\n",
    "<br>비용 함수:\n",
    "\n",
    "손실 함수는 하나의 훈련 예제에만 적용되지만, 전체 훈련 세트에 대한 성능을 평가하기 위해 비용 함수를 사용합니다.\n",
    "비용 함수는 모든 훈련 예제에 대한 손실 함수의 평균으로 정의됩니다: J(W, b) = -1/m * Σ[i=1 to m](y^(i) * log(y_hat^(i)) + (1-y^(i)) * log(1-y_hat^(i))).\n",
    "로지스틱 회귀 모델을 훈련시키기 위한 최종 목표는 이 비용 함수를 최소화하는 매개변수 W와 b를 찾는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d82659",
   "metadata": {},
   "source": [
    "### Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fe237f",
   "metadata": {},
   "source": [
    "Gradient descent는 로지스틱 회귀 모델에서 비용 함수를 최소화하기 위해 사용되는 알고리즘이다. 이 알고리즘의 목적은 파라미터 W와 b의 값들을 조정하여 전체 학습 세트에서의 오차를 최소화하는 것이다. 어디서 시작하더라도 전역 최소값을 향해 움직일 것이다.\n",
    "\n",
    "Gradient descent의 기본 아이디어는 파라미터의 현재 위치에서 함수의 기울기를 계산하고, 이 기울기의 방향으로 일정한 크기의 스텝을 취하는 것이다. 이러한 스텝의 크기는 학습률로 조정된다.\n",
    "\n",
    "기울기는 파라미터 값에 대한 비용 함수의 변화를 나타내며, 이는 미분을 사용하여 계산된다. 이를 통해 각 파라미터가 비용 함수에 어떤 영향을 미치는지 알 수 있다.\n",
    "\n",
    "최종적으로, gradient descent는 반복적으로 이러한 업데이트를 수행하여 비용 함수의 최소값을 찾는다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29eefec4",
   "metadata": {},
   "source": [
    "### Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cb0a09",
   "metadata": {},
   "source": [
    "실제로, 심층 학습에선 \"forward 함수\"와 \"backward 함수\"라는 두 가지 주요 함수를 통해 미적분과 관련된 모든 작업을 처리하게 된다. 이를 통해 복잡해 보이는 미적분의 개념을 이 함수들 내에서 간결하게 처리할 수 있다.\n",
    "\n",
    "도함수의 개념을 설명하기 위해, 함수의 기울기나 변화율을 나타내는 것으로 생각할 수 있다. '도함수'라는 용어는 처음 들어 복잡하게 들릴 수 있지만, 그것은 단순히 '기울기'라는 더 친숙한 용어로 생각될 수 있다.\n",
    "\n",
    "예를 들어, f(a)=3a라는 함수가 있다고 해보겠다. 이 함수에서 a의 값이 어디에서든 그 기울기는 항상 3이다. 즉, 만약 a 값을 아주 작은 값, 예를 들어 0.001만큼 증가시키면, f(a)는 그의 세 배인 0.003만큼 증가한다.\n",
    "이러한 변화율을 나타내는 도함수는 df(a)/da 로 표현될 수 있다. 이 수식은 \"f(a)에 대한 a의 도함수\"라는 것을 의미하며, 변수 a가 아주 작은 양만큼 변할 때 f(a)의 값이 얼마나 변하는지를 나타낸다.\n",
    "\n",
    "공식적으로 도함수는 변수를 무한히 작은 양만큼 바꾸었을 때 함수의 변화를 나타낸다. 이것은 함수의 순간적인 변화율을 \"무한히 작은 변화\"의 개념을 사용하여 측정하는 것을 의미한다.\n",
    "  #### 이후 세션은 미분에 대한 자세한 설명과 역전파알고리즘에 대한 개념과 이해에 대한 보충설명 그리고 선형회귀를 위한 경사하강에 대한 수식유도를 설명한다. 알잘딱으로 부족한개념은 요약이 아닌 자세한 공부를 해야함.(유튜브나 구글링,지피티 활용)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a648a4f5",
   "metadata": {},
   "source": [
    "<br> <br> <br> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad6eb03",
   "metadata": {},
   "source": [
    "# 2.Python and Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d86d86",
   "metadata": {},
   "source": [
    "# 3.Programming Assignments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

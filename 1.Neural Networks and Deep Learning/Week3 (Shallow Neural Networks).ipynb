{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e3e9a9a",
   "metadata": {},
   "source": [
    "# Shallow Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5882652a",
   "metadata": {},
   "source": [
    "### Neural Networks Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b464dccd",
   "metadata": {},
   "source": [
    "<pre>\n",
    "기본 개념: 지난 주에는 로지스틱 회귀에 대해 배웠고, 이번 주에는 신경망을 구현하는 방법에 대해 배울 것입니다.\n",
    "신경망의 구조: 신경망은 여러 시그모이드 유닛들을 함께 쌓아서 만들어진다. 이러한 유닛은 z와 a라는 두 단계의 계산으로 이루어진다.\n",
    "표기법: 신경망의 여러 계층을 나타내기 위해 대괄호 표기법을 사용한다. 예를 들어, [1]은 첫 번째 계층을 나타내고, [2]는 두 번째 계층을 나타낸다.\n",
    "계산 과정: x 피처와 w, b 파라미터를 사용하여 z_1을 계산한 다음, a_1을 계산하고, 다음 계층의 z와 a도 비슷한 방식으로 계산한다. a_2는 신경망의 최종 출력으로 y-hat과 동일하게 사용된다.\n",
    "역전파: 신경망에서는 로지스틱 회귀와 유사하게 역전파 방식을 사용하여 미분을 계산한다.\n",
    "전반적 개요: 기본적으로 신경망은 로지스틱 회귀를 여러 번 반복하여 구성된다.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5851c1c0",
   "metadata": {},
   "source": [
    "### Neural Network Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6f4b3b",
   "metadata": {},
   "source": [
    "<pre>\n",
    "신경망 구조: 신경망의 구조에는 입력 레이어, 숨겨진 레이어, 출력 레이어의 세 가지 주요 구성 요소가 있습니다.\n",
    "입력 레이어: 신경망의 입력을 포함하며, x1, x2, x3와 같은 입력 특성으로 구성됩니다.\n",
    "숨겨진 레이어: 훈련 세트에 직접 관찰되지 않는 노드 또는 유닛으로 구성된 레이어입니다. 이것이 \"숨겨진\"이라는 용어의 출처입니다.\n",
    "출력 레이어: 신경망의 예측값 y-hat을 생성하는 레이어입니다.\n",
    "표기법:\n",
    "A는 활성화를 나타내며 신경망의 각 레이어에서 전달되는 값을 나타냅니다.\n",
    "A^[0], A^[1] 등의 표기법은 각각의 레이어를 나타내며, A^[1]_1과 같이 아래 첨자를 사용하여 레이어 내의 특정 노드나 유닛을 표시할 수 있습니다.\n",
    "레이어 카운트: 입력 레이어는 공식 레이어로 카운트되지 않기 때문에, 예제의 신경망은 \"두 계층 신경망\"으로 불립니다.\n",
    "파라미터: 각 레이어는 연관된 파라미터 w와 b를 가지고 있습니다. 이 파라미터의 차원은 레이어의 유닛 수와 입력 특성의 수에 따라 다릅니다.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aa332b",
   "metadata": {},
   "source": [
    "### Computing a Neural Network's Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a0e1a3",
   "metadata": {},
   "source": [
    "<pre>\n",
    "산경망은 기본적으로 로지스틱 회귀를 여러 번 반복하는 것과 유사하다고 볼 수 있습니다. 은닉층의 각 노드는 두 단계의 계산을 수행합니다. 첫 번째 단계에서는 z=w^Tx+b를 계산하고, 두 번째 단계에서는 활성화 함수로서 시그모이드 함수를 사용하여 a 값을 계산합니다. 모든 노드에 대해 이러한 계산을 반복하며, 결과적으로 신경망의 전체 출력을 얻게 됩니다. 또한 이러한 계산을 효율적으로 수행하기 위해 벡터화하는 방법도 소개되었습니다. 따라서 신경망의 출력을 계산하기 위해 필요한 코드는 단 네 줄이며, 여러 훈련 예제에 대해 벡터화를 적용하여 전체 훈련 세트에 대한 출력도 한 번에 계산할 수 있습니다.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4f8b36",
   "metadata": {},
   "source": [
    "### Vectorizing Across Multiple Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7823e653",
   "metadata": {},
   "source": [
    "<pre>\n",
    "신경망에서의 \"벡터화\"는 여러 훈련 예제를 동시에 처리하기 위한 기술입니다. 훈련 데이터셋에 있는 각 예제에 대해 개별적으로 동일한 계산을 반복하는 대신, 벡터화를 통해 동일한 연산을 한 번에 수행하여 계산 효율을 크게 향상시킬 수 있습니다.\n",
    "\n",
    "이러한 벡터화의 주요 아이디어는 훈련 예제들을 컬럼으로 쌓아 큰 행렬을 형성하는 것입니다. 이렇게 구성된 행렬에서는 가로축이 다양한 훈련 예제를 나타내고, 세로축은 신경망의 노드 또는 특성을 나타냅니다. 이 행렬 형태를 사용하면, 각 예제에 대한 계산을 동시에 수행할 수 있으며, 이는 for 루프를 사용하여 예제별로 순차적으로 처리하는 것보다 훨씬 빠릅니다.\n",
    "\n",
    "벡터화의 중요한 장점 중 하나는 계산 속도의 향상입니다. 대부분의 최적화된 선형 대수 라이브러리나 계산 프레임워크들은 행렬 연산을 매우 효율적으로 처리할 수 있도록 설계되었습니다. 따라서 개별 예제들에 대한 계산을 벡터화된 행렬 연산으로 대체함으로써, 신경망의 학습과 예측 속도를 크게 개선할 수 있습니다.\n",
    "\n",
    "요약하면, 벡터화는 신경망에서의 계산 효율성을 크게 향상시키는 핵심 기술입니다. 여러 훈련 예제에 대해 동일한 연산을 동시에 수행함으로써, 처리 시간을 단축시키고 성능을 향상시킵니다.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe089425",
   "metadata": {},
   "source": [
    "### Explanation for Vectorized Implementation\n",
    "<pre>\n",
    "벡터화의 중요성: 신경망에서 여러 훈련 예제를 한 번에 처리하기 위해 벡터화를 사용합니다. 이는 학습 속도를 크게 향상시킵니다.\n",
    "\n",
    "훈련 예제의 스택: 훈련 예제들을 가로로 쌓아서 큰 행렬을 형성합니다. 그럼 이 행렬과 가중치 행렬을 곱하면 결과적으로 각 예제에 대한 연산 결과가 행렬의 열로 나타납니다.\n",
    "\n",
    "Python Broadcasting: Python의 broadcasting 기능을 사용하면, 행렬에 스칼라 값을 쉽게 더할 수 있습니다. 이를 통해 바이어스 b 값을 각 열에 쉽게 더할 수 있습니다.\n",
    "\n",
    "벡터화의 유사성: 신경망의 각 계층에서 수행하는 연산은 크게 다르지 않습니다. 이는 신경망의 각 계층이 비슷한 연산을 반복적으로 수행한다는 것을 의미합니다.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd6d9ae",
   "metadata": {},
   "source": [
    "### Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2704760",
   "metadata": {},
   "source": [
    "<pre>\n",
    "\n",
    "활성화 함수의 역할: 활성화 함수는 신경망의 히든 레이어와 출력 유닛에서 사용됩니다. 활성화 함수를 사용하는 주요 이유는 비선형성을 도입하여 신경망이 더 복잡한 패턴을 학습하도록 하는 것입니다.\n",
    "\n",
    "●시그모이드 함수 (Sigmoid)\n",
    "식: a = 1 / (1 + exp(-z))\n",
    "범위: 0과 1 사이\n",
    "특징: 시그모이드 함수는 그 출력값이 0과 1 사이로 제한되기 때문에 이진 분류 문제의 출력 계층에서 주로 사용됩니다. 그러나, 중간 계층에서는 tanh 함수가 성능 면에서 종종 우월하다고 알려져 있습니다.\n",
    "\n",
    "●하이퍼볼릭 탄젠트 함수 (Tanh)\n",
    "식: a = (exp(z) - exp(-z)) / (exp(z) + exp(-z))\n",
    "범위: -1과 1 사이\n",
    "특징: tanh 함수는 출력 범위가 -1에서 1 사이입니다. 이로 인해, 데이터의 평균 출력이 0 근처로 중심화되는 효과가 있어 학습이 보다 원활하게 진행될 수 있습니다. 숨겨진 계층에서 주로 사용됩니다.\n",
    "\n",
    "●ReLU (Rectified Linear Unit)\n",
    "식: a = max(0, z)\n",
    "특징: ReLU는 현대 딥 러닝에서 가장 인기 있는 활성화 함수 중 하나입니다. 이 함수는 z가 양수일 때의 경사도는 1, 음수일 때는 0이라는 특징이 있습니다. 이 특징 때문에 학습 속도가 상대적으로 빠르며, 대부분의 딥 러닝 모델에서 잘 작동합니다.\n",
    "\n",
    "●Leaky ReLU\n",
    "식: a = max(0.01 * z, z)\n",
    "특징: Leaky ReLU는 ReLU의 변형된 형태입니다. 기존의 ReLU에서 z가 음수일 때의 경사도 문제를 해결하고자, 음수 영역에서 약간의 경사도(0.01 등)를 부여하는 것이 특징입니다.\n",
    "\n",
    "이러한 활성화 함수들은 신경망에서 비선형성을 도입하는 주요 수단으로 사용됩니다. 만약 활성화 함수를 사용하지 않으면, 신경망은 기본적으로 선형 연산만을 수행하게 되어 복잡한 패턴이나 관계를 학습하는 데에 한계가 있게 됩니다.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cc332f",
   "metadata": {},
   "source": [
    "### Why do you need Non-Linear Activation Functions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11dfd2e",
   "metadata": {},
   "source": [
    "<pre>\n",
    "●선형 활성화 함수의 한계:\n",
    "신경망의 전파 방정식에서 활성화 함수 g(z)를 제거하면 a=z가 됩니다. 이는 선형 활성화 함수 또는 아이덴티티 활성화 함수로 불립니다. 이 경우 입력 x와 출력 y (또는 y-hat) 사이의 관계는 단순한 선형 관계가 됩니다.\n",
    "여러 층의 신경망에서도 선형 활성화 함수만 사용하면 각 층이 결국 하나의 선형 변환으로 단순화되어 복잡한 함수를 표현할 수 없게 됩니다.\n",
    "\n",
    "●선형 활성화 함수와 로지스틱 회귀:\n",
    "만약 우리가 한 은닉층에 선형 활성화 함수를 사용하고 출력층에 시그모이드 함수를 사용한다면, 이 신경망은 표준 로지스틱 회귀와 같은 표현력을 가지게 됩니다. 이는 선형 활성화 함수의 한계 때문입니다.\n",
    "\n",
    "●비선형 활성화 함수의 중요성:\n",
    "은닉층에 비선형 활성화 함수를 도입하면 신경망은 다양한 패턴과 복잡한 함수를 학습할 수 있게 됩니다. 따라서 신경망에서 은닉층의 활성화 함수는 비선형이어야 합니다.\n",
    "\n",
    "●회귀 문제에서의 선형 활성화 함수:\n",
    "회귀 문제에서는 예측값이 연속적인 실수 값이어야 하기 때문에 출력 층에서 선형 활성화 함수를 사용하는 것이 타당할 수 있습니다. 하지만 은닉층에서는 여전히 비선형 활성화 함수를 사용해야 합니다.\n",
    "결론적으로, 비선형 활성화 함수는 신경망이 복잡한 패턴과 함수를 학습하는 데 필수적입니다. 선형 활성화 함수만을 사용하면 신경망의 표현력이 크게 제한됩니다.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c879bc1",
   "metadata": {},
   "source": [
    "### Derivatives of Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f07d589",
   "metadata": {},
   "source": [
    "### 1. 시그모이드 활성화 함수\n",
    "\n",
    "**시그모이드 함수**는 다음과 같이 정의됩니다:\n",
    "\n",
    "$[ g(z) = \\frac{1}{1 + e^{-z}} $]\n",
    "\n",
    "이 함수의 도함수는:\n",
    "\n",
    "$[ g'(z) = g(z)(1-g(z)) $]\n",
    "\n",
    "이 식에서, $( g'(z) $)는 함수 $( g $)의 도함수를 나타냅니다. 시그모이드 함수의 출력값은 0과 1 사이이므로, 도함수의 값도 0과 1 사이에 있습니다. $( z $)가 큰 양수이거나 큰 음수일 경우 시그모이드 함수의 기울기는 0에 가까워집니다.\n",
    "\n",
    "### 2. 하이퍼볼릭 탄젠트 활성화 함수\n",
    "\n",
    "**하이퍼볼릭 탄젠트 함수**는 다음과 같이 정의됩니다:\n",
    "\n",
    "$[ g(z) = \\tanh(z) $]\n",
    "\n",
    "이 함수의 도함수는:\n",
    "\n",
    "$[ g'(z) = 1 - (g(z))^2 $]\n",
    "\n",
    "여기서도 마찬가지로, 하이퍼볼릭 탄젠트 함수의 출력값은 -1과 1 사이이므로, 도함수의 값도 0과 1 사이에 있습니다.\n",
    "\n",
    "### 3. ReLU 활성화 함수\n",
    "\n",
    "**ReLU 함수**는 다음과 같이 정의됩니다:\n",
    "\n",
    "$[ g(z) = \\max(0, z) $]\n",
    "\n",
    "이 함수의 도함수는 $( z $)가 0보다 클 때 1, 0보다 작을 때 0입니다. $( z = 0 $)일 때는 기술적으로 정의되지 않지만, 실제로 구현할 때 0 또는 1로 설정해도 됩니다.\n",
    "\n",
    "### 4. Leaky ReLU 활성화 함수\n",
    "\n",
    "**Leaky ReLU 함수**는 다음과 같이 정의됩니다:\n",
    "\n",
    "$[ g(z) = \\max(0.01z, z) $]\n",
    "\n",
    "이 함수의 도함수는 $( z $)가 0보다 클 때 1, 0보다 작을 때 0.01입니다.\n",
    "\n",
    "이렇게 각 활성화 함수에 대한 도함수를 알게 되면, 이를 기반으로 신경망에서 역전파를 통해 가중치를 업데이트하는 데 필요한 기울기를 계산할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3431123a",
   "metadata": {},
   "source": [
    "### Gradient Descent for Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1cdf91",
   "metadata": {},
   "source": [
    "1. **신경망 구조**\n",
    "   - 단일 은닉층의 신경망에서는 주로 W1, B1, W2, B2라는 매개변수를 사용합니다.\n",
    "   - 이들 매개변수의 차원은 입력 특징, 은닉 유닛, 그리고 출력 유닛의 수에 따라 달라집니다.\n",
    "     \n",
    "     1-1. **입력 특징의 수 (NX 또는 N0)**\n",
    "       - 입력 특징의 수는 데이터가 신경망에 주어지는 특징의 개수입니다. 예를 들어, 28x28 픽셀의 이미지를 처리한다면, 입력 특징의 수는 784개가 됩니다.\n",
    "       - 첫 번째 가중치 행렬 $( W1 $)의 열의 수는 이 입력 특징의 수와 일치합니다. $( W1 $)은 입력층과 은닉층 사이의 연결을 나타내므로, 각 입력 특징은 해당 가중치와 연결됩니다.\n",
    "\n",
    "        1-2. **은닉 유닛의 수 (N1)**\n",
    "       - 은닉 유닛의 수는 은닉층에서의 뉴런의 수입니다. \n",
    "       - $( W1 $)의 행의 수는 은닉 유닛의 수와 같습니다. 왜냐하면 각 은닉 유닛은 입력 특징 전체와 연결되기 때문입니다.\n",
    "       - 바이어스 벡터 $( W1 $)의 차원은 은닉 유닛의 수와 같습니다.\n",
    "\n",
    "        1-3. **출력 유닛의 수 (N2)**\n",
    "       - 출력 유닛의 수는 신경망의 출력층에서의 뉴런의 수입니다.\n",
    "       - 두 번째 가중치 행렬 $( W2 $)의 행의 수는 출력 유닛의 수와 일치합니다. 각 출력 유닛은 은닉층의 모든 유닛과 연결되기 때문입니다.\n",
    "       - 바이어스 벡터 $( B2 $)의 차원도 출력 유닛의 수와 동일합니다.\n",
    "\n",
    "        이렇게 입력, 은닉, 출력층의 유닛 수에 따라 매개변수의 차원이 결정되는 이유는, 각 층의 유닛들이 서로 어떻게 연결되는지에 따라 달라집니다. 가중치 행렬의 각 원소는 두 뉴런 간의 연결 강도를 나타내며, 바이어스는 각 뉴런의 활성화 임계값을 조절합니다. 따라서 신경망의 구조와 크기에 따라 이 매개변수들의 차원이 결정됩니다.\n",
    "\n",
    "\n",
    "2. **비용 함수**\n",
    "   - 주로 이진 분류를 다루기 위한 비용 함수를 사용하며, 이 함수는 신경망의 예측값 $(Y\\hat{} $)과 실제 레이블 Y 간의 손실을 평가합니다.\n",
    "   - 비용함수는 알고리즘의 예측 출력과 실제 정답 간의 차이를 측정하는 함수입니다. 이 차이를 최소화하는 것이 신경망 학습의 목표입니다.\n",
    "\n",
    "    비용함수를 이해하기 위해 로지스틱 회귀에서 사용되는 비용함수를 먼저 생각해볼 수 있습니다. 로지스틱 회귀에서는 아래와 같은 비용함수를 사용합니다:\n",
    "    $[ J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)} \\log(h_{\\theta}(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_{\\theta}(x^{(i)}))] $] \n",
    "        여기서 $( h_{\\theta}(x) $)는 로지스틱 함수의 출력값(즉, 예측값)을 의미하며, $( y $)는 실제 정답 레이블을 나타냅니다.\n",
    "\n",
    "    - 신경망에서도 위의 로지스틱 회귀의 비용함수를 사용할 수 있습니다. 실제로, 이진 분류를 하는 신경망의 마지막 층에서는 시그모이드 활성화 함수가 사용되며, 이는 로지스틱 회귀의 출력과 동일합니다.\n",
    "\n",
    "        - 신경망의 비용함수는 다음과 같습니다:\n",
    "\n",
    "        - $[ J(W, b) = - \\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)} \\log(a^{[L] (i)}) + (1 - y^{(i)}) \\log(1 - a^{[L] (i)})] $]\n",
    "        여기서 $( a^{[L]} $)는 신경망의 마지막 층에서의 활성화 함수의 출력을 의미합니다. $( W $)와 $( b $)는 신경망의 가중치와 바이어스를 나타내며, 이들을 조절하여 비용함수를 최소화하는 것이 학습의 목적입니다.\n",
    "           다중 클래스 분류 문제의 경우, 비용함수는 약간 수정되어 다중 클래스에 적합한 크로스 엔트로피 비용함수로 사용됩니다.\n",
    "           결국, 비용함수는 신경망의 출력과 실제 정답 간의 차이를 나타내는 측정치입니다. 학습 과정에서는 이 차이를 최소화하기 위해 매개변수를 지속적으로 조절하게 됩니다.\n",
    "\n",
    "3. **경사 하강법**\n",
    "   - 먼저, 매개변수를 무작위로 초기화해야 합니다. 모든 매개변수를 0으로 초기화하는 것은 좋지 않습니다.\n",
    "   - 매 단계에서, 신경망의 예측을 계산하고, 해당 예측에 대한 비용 함수의 편미분을 계산합니다.\n",
    "   - 편미분 값을 사용하여 매개변수를 업데이트합니다.\n",
    "\n",
    "4. **순전파 (Forward Propagation)**\n",
    "   - 입력에서 출력까지의 계산은 주로 선형 변환과 활성화 함수를 사용하여 수행됩니다.\n",
    "   - 이를 통해 최종 출력값을 얻습니다.\n",
    "\n",
    "5. **역전파 (Backpropagation)**\n",
    "   - 이는 신경망의 오차를 역방향으로 전파하여 각 매개변수에 대한 비용 함수의 편미분을 계산하는 과정입니다.\n",
    "   - 이 편미분들은 매개변수를 업데이트하는 데 사용됩니다.\n",
    "\n",
    "   \n",
    "6. **손실함수 (Loss Function)**\n",
    "   - 이진 교차 엔트로피(Binary Cross-Entropy) \n",
    "   \n",
    "   $[ L(y, \\hat{y}) = - \\left( y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y}) \\right) $]\n",
    "   - $( y $)는 실제 값이며 0 또는 1의 값을 갖습니다\n",
    "   - $( \\hat{y} $)는 예측된 확률값으로 0과 1 사이의 값을 갖습니다.\n",
    "   이 수식의 의미를 간단하게 설명하자면:\n",
    "   - $( y = 1 $)일 때, 손실은 $( -\\log(\\hat{y}) $)가 됩니다. 이는 예측된 확률 $( \\hat{y} $)가 1에 가까울수록 손실이 0에 가까워지며, 0에 가까울수록 손실이 크게 증가한다는 것을 의미합니다.\n",
    "   - $( y = 0 $)일 때, 손실은 $( -\\log(1 - \\hat{y}) $)가 됩니다. 이는 예측된 확률 $( \\hat{y} $)가 0에 가까울수록 손실이 0에 가까워지며, 1에 가까울수록 손실이 크게 증가한다는 것을 의미합니다.\n",
    "   따라서, 이진 교차 엔트로피 손실 함수는 실제 값과 예측 값이 일치할수록 손실이 작아지며, 불일치할수록 손실이 크게 증가합니다. 이를 통해 모델은 손실을 최소화하도록 예측 확률을 조정하게 됩니다.\n",
    "\n",
    "결론적으로, 이러한 순전파와 역전파 알고리즘을 구현하면 신경망의 매개변수를 학습시킬 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d9e0cc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cb7e2c6",
   "metadata": {},
   "source": [
    "### Backpropagation Intuition "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf13b58d",
   "metadata": {},
   "source": [
    "1. **로지스틱 회귀에서의 역전파**:\n",
    "   - 먼저 로지스틱 회귀에서는 Z, A, 그리고 손실을 계산하는 순전파 과정이 있습니다.\n",
    "   - 손실 함수 L의 미분을 통해 da를 구하면, 이를 바탕으로 dz, dw, db를 차례로 구합니다.\n",
    "\n",
    "2. **신경망에서의 역전파**:\n",
    "   - 로지스틱 회귀와 유사하지만, 중간에 은닉층이 있어 한 단계를 더 거칩니다.\n",
    "   - 순전파에서는 입력 x를 기반으로 z1, a1, z2, a2, 손실을 순차적으로 계산합니다.\n",
    "   - 역전파에서는 이러한 계산을 역순으로 수행하여 파라미터에 대한 변화를 구합니다.\n",
    "\n",
    "\n",
    "3. **행렬의 차원 맞추기**:\n",
    "   - 역전파를 구현할 때 중요한 것 중 하나는 행렬의 차원을 올바르게 맞추는 것입니다. 행렬의 차원이 일치하도록 구현하면 버그를 크게 줄일 수 있습니다.\n",
    "\n",
    "\n",
    "4. **벡터화와 배치 처리**:\n",
    "   - 실제로 우리는 하나의 훈련 예제가 아니라 여러 훈련 예제들을 동시에 처리하기 위해 벡터화를 사용합니다.\n",
    "   - 벡터화된 역전파의 방정식도 다루어졌습니다.\n",
    "\n",
    "\n",
    "5. **정리**:\n",
    "   - 역전파 알고리즘의 유도는 딥러닝 내에서도 수학적으로 복잡한 부분 중 하나입니다.\n",
    "   - 이 동영상의 내용을 이해한다면, 실제로 이 알고리즘을 구현하고 조정하는 데 필요한 기본적인 직관을 얻을 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716a9a89",
   "metadata": {},
   "source": [
    "### Random Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9983637",
   "metadata": {},
   "source": [
    "### 왜 가중치를 0으로 초기화하는 것이 문제인가?\n",
    "\n",
    "1. 모든 가중치가 0으로 초기화되면, 숨겨진 유닛 간에 대칭성이 발생합니다. 이것은 모든 숨겨진 유닛이 동일한 출력을 생성한다는 것을 의미합니다.\n",
    "2. 그 결과, 역전파 과정에서 동일한 가중치 업데이트가 발생하므로, 학습 과정 중에도 이러한 대칭성이 깨지지 않습니다.\n",
    "3. 결국, 여러 개의 숨겨진 유닛이 있다 해도 모두 동일한 기능을 수행하기 때문에, 그것들이 실제로 유용하지 않게 됩니다.\n",
    "\n",
    "### 해결 방법은?\n",
    "\n",
    "가중치를 작은 무작위 값으로 초기화하는 것입니다. 이렇게 하면 각 숨겨진 유닛이 다르게 동작하기 시작하여 대칭성 문제를 피할 수 있습니다.\n",
    "\n",
    "### 왜 작은 값으로 초기화하는가?\n",
    "\n",
    "만약 큰 값을 사용하면, 활성화 함수(예: tanh 또는 sigmoid)가 포화 상태가 되어 학습 속도가 느려질 수 있습니다. 초기 가중치가 너무 크면 활성화 함수의 출력이 너무 크거나 작아져서 기울기가 작아지게 됩니다.\n",
    "\n",
    "### 요약\n",
    "\n",
    "- 0으로 가중치를 초기화하면, 모든 숨겨진 유닛이 동일하게 동작합니다.\n",
    "- 가중치를 작은 무작위 값으로 초기화하여 이 문제를 해결할 수 있습니다.\n",
    "- 너무 큰 값을 사용하여 가중치를 초기화하면 학습 속도가 느려질 수 있으므로, 일반적으로 작은 값을 사용합니다.\n",
    "\n",
    "이 내용은 신경망을 초기화하고 학습하는 방법에 대한 기본적인 가이드라인입니다. 실제로는 다양한 방법과 기법이 있으므로, 주어진 문제와 데이터에 따라 적절한 초기화 방법을 선택하는 것이 중요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ed7ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6b41552",
   "metadata": {},
   "source": [
    "# Setting up your Machine Learning Application "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baabee5",
   "metadata": {},
   "source": [
    "### Train / Dev / Test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6f0094",
   "metadata": {},
   "source": [
    "1. **디프 러닝의 실용적인 측면**\n",
    "\n",
    "    - 딥 러닝에서는 많은 결정들을 내려야 합니다: 신경망의 계층 수, 각 계층의 숨겨진 단위의 수, 학습 속도, 활성화 함수 등을 결정해야 합니다.\n",
    "    - 초기 시도에서 모든 것을 올바르게 선택하는 것은 거의 불가능합니다. 그렇기 때문에 응용 기계 학습은 반복적인 프로세스입니다.\n",
    "  \n",
    "2. **현대 딥 러닝의 성공**\n",
    "    - 딥 러닝은 자연어 처리, 컴퓨터 비전, 음성 인식, 구조화된 데이터에서 큰 성공을 거두었습니다.\n",
    "    - 다양한 분야에서의 경험이 다른 분야로 그대로 전환되지 않을 수 있습니다. 따라서 적절한 하이퍼파라미터를 처음부터 정확하게 추측하는 것은 거의 불가능합니다.\n",
    "  \n",
    "3. **데이터 세트 설정**\n",
    "    - 전통적으로 사용 가능한 모든 데이터를 훈련, 검증(또는 개발), 테스트 세트로 나눕니다.\n",
    "    - 큰 데이터 세트를 가지고 있다면, 검증 및 테스트 세트의 크기를 줄일 수 있습니다. \n",
    "\n",
    "4. **불일치된 훈련 및 테스트 분포**\n",
    "    - 딥 러닝에서는 훈련 데이터와 테스트 데이터가 다른 분포를 가질 수 있습니다.\n",
    "    - 중요한 것은 개발 세트와 테스트 세트가 같은 분포를 가져야 한다는 것입니다.\n",
    "\n",
    "5. **테스트 세트의 필요성**\n",
    "    - 모든 상황에서 테스트 세트가 필요한 것은 아닙니다. \n",
    "    - 테스트 세트의 목적은 최종 모델의 성능에 대한 편향되지 않은 추정을 제공하는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f31064",
   "metadata": {},
   "source": [
    "### VideoBias / Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66f713e",
   "metadata": {},
   "source": [
    "1. **바이어스(Bias)와 분산(Variance)의 이해**:\n",
    "   - 바이어스는 모델이 학습 데이터에 대해서 얼마나 잘 일반화되지 않는지를 나타냅니다. 높은 바이어스를 가진 모델은 데이터의 패턴을 충분히 학습하지 못해 과소적합(underfitting)이 발생합니다.\n",
    "   - 분산은 모델이 학습 데이터의 작은 변화에 얼마나 민감한지를 나타냅니다. 높은 분산을 가진 모델은 학습 데이터의 노이즈까지 너무 잘 학습해버려 과대적합(overfitting)이 발생합니다.\n",
    "\n",
    "2. **딥러닝에서의 바이어스/분산 트레이드오프**:\n",
    "   - 과거에는 바이어스와 분산 사이에는 트레이드오프 관계가 있다고 여겨졌습니다. 즉, 바이어스를 줄이면 분산이 증가하고, 분산을 줄이면 바이어스가 증가하는 경향이 있었습니다.\n",
    "   - 그러나 딥러닝에서는 이런 트레이드오프가 크게 관찰되지 않습니다. 복잡한 네트워크와 충분한 데이터를 이용하면 동시에 바이어스와 분산을 줄일 수 있습니다.\n",
    "\n",
    "3. **학습 오차와 개발 오차**:\n",
    "   - 학습 오차와 개발 오차를 비교함으로써 바이어스와 분산의 문제를 진단할 수 있습니다.\n",
    "   - 학습 오차가 높으면 바이어스가 높은 것이고, 학습 오차에 비해 개발 오차가 훨씬 높으면 분산이 높은 것입니다.\n",
    "\n",
    "4. **베이즈 오차**:\n",
    "   - 최적의 분류기로도 달성할 수 없는 최소의 오차를 말합니다. 이 값이 높은 경우, 바이어스와 분산을 진단하는 방법이 다를 수 있습니다.\n",
    "\n",
    "요약하자면, 바이어스와 분산은 머신러닝 모델의 성능을 평가하고 문제점을 진단하는 데 중요한 요소입니다. 이를 통해 모델이 학습 데이터에 과소적합하거나 과대적합하는지 알 수 있습니다. 따라서 바이어스와 분산의 개념을 잘 이해하는 것은 머신러닝의 성능을 향상시키는 데 도움이 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ae45da",
   "metadata": {},
   "source": [
    "### Basic Recipe for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c09aa1f",
   "metadata": {},
   "source": [
    "머신 러닝 모델을 학습할 때, 우리는 주로 두 가지 큰 문제, 즉 **편향(bias)**과 **분산(variance)**을 직면합니다.\n",
    "\n",
    "1. **편향 문제(Bias Problem)**\n",
    "   - 편향이 높다는 것은 모델이 훈련 데이터를 잘 학습하지 못한다는 것을 의미합니다.\n",
    "   - 해결책:\n",
    "     - 더 큰 네트워크(더 많은 히든 레이어나 유닛) 사용\n",
    "     - 더 오래 학습\n",
    "     - 다른 최적화 알고리즘 사용\n",
    "     - 적합한 신경망 구조 찾기(이것은 실험적으로 접근해야 할 수 있음)\n",
    "\n",
    "2. **분산 문제(Variance Problem)**\n",
    "   - 분산이 높다는 것은 모델이 훈련 데이터에는 잘 맞지만 새로운 데이터(개발 세트)에는 잘 일반화하지 못한다는 것을 의미합니다.\n",
    "   - 해결책:\n",
    "     - 더 많은 훈련 데이터 수집\n",
    "     - 정규화(regularization) 사용\n",
    "     - 적합한 신경망 구조 찾기\n",
    "\n",
    "과거에는 편향과 분산 사이에 균형을 잡아야 하는 \"편향-분산 트레이드오프\"라는 개념이 있었습니다. 그러나 현대 딥 러닝에서는 큰 네트워크를 학습시키거나 더 많은 데이터를 얻는 방법으로 편향만 줄이거나 분산만 줄이는 방법을 갖게 되었습니다.\n",
    "\n",
    "따라서, 주어진 문제에 따라 편향 또는 분산 문제를 진단하고 적절한 접근 방식을 선택하는 것이 중요합니다. \n",
    "\n",
    "정규화는 네트워크가 과적합(overfitting)되는 것을 방지하는 데 유용한 기법입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01c7bf6",
   "metadata": {},
   "source": [
    "# Regularizing your Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf38a54",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d63fcc7b",
   "metadata": {},
   "source": [
    "신경망이 데이터에 과적합(overfitting)되는 것을 방지하기 위한 일반적인 방법 중 하나는 정규화(regularization)입니다. 정규화는 모델의 복잡성을 제한하여 훈련 데이터에 너무 딱 맞게 학습되는 것을 막습니다.\n",
    "\n",
    "**로지스틱 회귀에서의 정규화:**\n",
    "로지스틱 회귀에서의 비용 함수 $( J $)를 최소화하려고 합니다. 정규화를 추가하려면 비용 함수에 $( \\frac{\\lambda}{2m} \\times ||w||^2 $) 항을 추가합니다. 여기서 $( $lambda $)는 정규화 파라미터로, 너무 큰 값을 갖는 가중치 $(w)를 제한하기 위해 사용됩니다. \n",
    "\n",
    "$( ||w||^2 $)는 L2 노름이라고도 불리며, 이는 가중치의 각 요소를 제곱한 값의 합입니다. 이 정규화 방법은 L2 정규화라고도 합니다. L1 정규화의 경우 가중치의 절대값을 사용합니다.\n",
    "\n",
    "**신경망에서의 정규화:**\n",
    "신경망에서는 여러 층의 가중치 $( w[1], w[2], ... $)가 있습니다. 정규화를 추가하려면 모든 가중치 행렬에 대한 제곱합을 비용 함수에 추가합니다. 이 값은 프로베니우스 노름(Frobenius norm)으로 알려져 있으며, 행렬의 모든 요소의 제곱의 합입니다.\n",
    "\n",
    "정규화를 적용한 후 경사 하강법을 사용하여 가중치를 업데이트할 때 $( w $)에 대한 업데이트는 $( dw +frac{\\lambda}{m} \\times w $)에 비례하는 값으로 변경됩니다.\n",
    "\n",
    "이러한 정규화 방법을 사용하면 가중치가 감소하는 경향이 있습니다. 따라서 L2 정규화는 종종 'weight decay'라고도 불립니다.\n",
    "\n",
    "**왜 정규화는 과적합을 방지하는가?**\n",
    "정규화는 모델의 가중치를 제한하여 모델이 훈련 데이터에 과도하게 적응하는 것을 방지합니다. 가중치가 작아지면 모델의 복잡성이 감소하게 되므로 일반화 능력이 향상됩니다.\n",
    "\n",
    "마지막으로, $( $lambda $)는 하이퍼파라미터로서 개발 세트나 교차 검증을 통해 조정해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972e18ce",
   "metadata": {},
   "source": [
    "### Why Regularization Reduces Overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafb5c13",
   "metadata": {},
   "source": [
    "정규화는 기본적으로 모델이 학습 데이터에 너무 맞춰져 있어서 새로운 데이터에 대한 성능이 떨어지는 과적합 문제를 해결하기 위한 방법 중 하나입니다.\n",
    "\n",
    "1. **과적합의 예시**\n",
    "   - 고분산과 고편향: 고분산은 모델이 학습 데이터에 지나치게 적응하여 복잡한 경계선을 그리는 것을 의미하며, 고편향은 모델이 너무 단순해서 데이터의 패턴을 잡아내지 못하는 것을 의미합니다.\n",
    "   \n",
    "2. **정규화의 동작 원리**\n",
    "   - 정규화는 비용 함수에 패널티 항을 추가하여 모델의 가중치가 너무 큰 값을 가지지 않도록 합니다.\n",
    "   - L2 정규화는 Frobenius norm(프로베니우스 노름)을 사용하여 가중치의 크기를 패널티로 부과합니다.\n",
    "   \n",
    "3. **정규화가 과적합을 방지하는 직관적 이해**\n",
    "   - 큰 정규화 값 (λ)을 사용하면 가중치가 작아집니다. 따라서 신경망의 복잡성이 줄어들어 데이터에 과도하게 적응하는 것을 방지할 수 있습니다.\n",
    "   - 실제로 모든 은닉 유닛이 0으로 설정되는 것은 아니지만, 각 은닉 유닛의 효과가 작아져 신경망이 단순화됩니다.\n",
    "   \n",
    "4. **tan h 활성화 함수와의 관계**\n",
    "   - tan h 함수는 z의 값이 작을 때 선형과 유사합니다. 따라서 z의 값이 작으면 신경망은 선형적인 동작을 합니다.\n",
    "   - 큰 정규화 값 때문에 가중치와 z가 작아질 때, 신경망은 복잡한 비선형 경계선 대신 단순한 선형 경계선을 갖게 됩니다.\n",
    "   \n",
    "5. **구현 팁**\n",
    "   - 정규화를 사용할 때 비용 함수는 기존의 정의에서 추가된 패널티 항을 포함하게 됩니다.\n",
    "   - 그래디언트 디센트를 디버깅할 때는 이 새로운 비용 함수 정의를 사용해야 합니다.\n",
    "\n",
    "정리하면, 정규화는 모델의 가중치가 너무 큰 값을 가지는 것을 방지하여 모델이 학습 데이터에 과도하게 적응하는 것을 방지하며, 이를 통해 새로운 데이터에 대한 모델의 성능을 향상시키는 효과가 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1abc882",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

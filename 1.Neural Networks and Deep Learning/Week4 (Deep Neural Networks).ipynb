{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9b93f33",
   "metadata": {},
   "source": [
    "# Deep L-layer Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27242a5f",
   "metadata": {},
   "source": [
    "1. **심층 신경망이란?**\n",
    "   - 로지스틱 회귀(logistic regression)는 매우 \"얕은\" 모델로 간주됩니다.\n",
    "   - 이와 대조적으로 더 많은 히든 레이어(hidden layer)를 포함하는 모델은 \"깊은\" 모델로 간주됩니다.\n",
    "   - 예를 들어, 5개의 히든 레이어를 포함하는 모델은 깊은 모델입니다.\n",
    "   - 실제로 로지스틱 회귀는 한 층 신경망이라고 할 수 있습니다.\n",
    "\n",
    "2. **신경망의 층 계산**\n",
    "   - 신경망의 층을 계산할 때 입력 층(input layer)은 포함되지 않습니다.\n",
    "   - 예를 들어, 하나의 히든 레이어만 있는 신경망은 2층 신경망으로 간주됩니다.\n",
    "\n",
    "3. **표기법**\n",
    "   - $( L $): 신경망의 전체 레이어 수\n",
    "   - $( n^{[l]} $): $( l $) 레이어의 노드 또는 유닛 수\n",
    "   - $( a^{[l]} $): $( l $) 레이어의 활성화 값(activations)\n",
    "   - $( $^{[l]} $), $( b^{[l]} $): 각각 $( l $) 레이어의 가중치(weights)와 편향(bias)\n",
    "\n",
    "예를 들어 3개의 히든 레이어와 1개의 출력 레이어를 가진 신경망에서:\n",
    "- $( L = 4 $) (입력 레이어 제외)\n",
    "- $( n^{[1]} = 5, n^{[2]} = 5, n^{[3]} = 3, n^{[4]} = 1 $\\) \n",
    "- $( a^{[0]} $)는 입력 특성 $( x $)와 같으며, 마지막 레이어의 활성화 $( a^{[L]} $)는 예측 값 $( \\hat{y} $)와 같습니다.\n",
    "\n",
    "요약하면, 심층 신경망은 여러 히든 레이어를 가진 신경망을 의미하며, 각 레이어에는 여러 노드나 유닛이 있습니다. 이러한 신경망의 구조와 연산을 설명하기 위한 다양한 표기법이 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543ad86a",
   "metadata": {},
   "source": [
    "# Forward Propagation in a Deep Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d5bf37f7",
   "metadata": {},
   "source": [
    "1. **단일 학습 예제에 대한 순방향 전파**: 먼저 한 개의 학습 예제 $( x $)에 대한 순방향 전파를 살펴봅니다. \n",
    "   - 첫 번째 계층의 활성화는 다음과 같이 계산됩니다: $( z^1 = w^1x + b^1 $). 그 후, 해당 계층의 활성화 함수 $( g $)를 적용해 $( a^1 = g(z^1) $)를 얻습니다.\n",
    "   - 이와 비슷한 방식으로 두 번째 계층의 활성화를 계산하면, $( z^2 = w^2a^1 + b^2 $) 그리고 $( a^2 = g(z^2) $) 입니다.\n",
    "   - 이런 방식으로 계속 진행하여 마지막 계층까지 도달합니다.\n",
    "   - 일반적인 순방향 전파 규칙은 $( z^l = w^la^{l-1} + b^l $) 및 $( a^l = g(z^l) $)로 표현됩니다.\n",
    "   \n",
    "2. **벡터화된 순방향 전파**: 전체 학습 세트에 대한 순방향 전파를 한번에 수행하려면 벡터화가 필요합니다.\n",
    "   - 각 학습 예제에 대한 z와 a 값을 벡터로 쌓아 큰 행렬을 형성합니다. 예를 들어, $( Z^1 $)은 모든 학습 예제에 대한 $( z^1 $) 값을 갖습니다.\n",
    "   - 순방향 전파 공식은 거의 동일하며, 벡터 $( x $) 대신 행렬 $( X $)를 사용합니다. (이때 $( X $)는 $( A^0 $)와 동일합니다.)\n",
    "   - 결과적으로, 전체 학습 세트에 대한 예측 값 $( $hat{y} $)는 $( A^L $)로 표현됩니다.\n",
    "\n",
    "3. **구현 시 주의할 점**: 깊은 신경망을 구현할 때, 버그가 없는 구현을 위해 행렬의 차원에 대해 체계적으로 생각하는 것이 중요합니다. 이는 코드 디버깅 시 매우 유용합니다. 순방향 전파를 구현할 때, 각 계층에 대한 활성화를 계산하기 위해 명시적인 For 루프가 필요합니다. 이것은 대부분의 신경망 구현에서 피할 수 없는 부분입니다.\n",
    "\n",
    "결론적으로, 깊은 신경망의 순방향 전파는 기본적으로 하나의 숨겨진 계층을 가진 신경망에서 본 내용의 반복입니다. 행렬의 차원을 정확히 파악하는 것은 구현 시 중요한 점입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fee9c3",
   "metadata": {},
   "source": [
    "# Getting your Matrix Dimensions Right"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27773f1d",
   "metadata": {},
   "source": [
    "신경망은 여러 계층으로 구성됩니다. 이 예에서는 5개의 계층(4개의 은닉 계층과 1개의 출력 계층)으로 구성되어 있습니다. 이 때 각 계층에 있는 유닛(노드)의 수는 nL로 표현하며, 주어진 예에서는 다음과 같이 나타낼 수 있습니다:\n",
    "- n1 = 3\n",
    "- n2 = 5\n",
    "- n3 = 4\n",
    "- n4 = 2\n",
    "- n5 = 1\n",
    "- n0 (입력 계층) = 2\n",
    "\n",
    "신경망에서 주요한 연산 중 하나는 각 계층에서의 가중치 W와 입력 X를 곱하여 Z를 계산하는 것입니다. Z의 차원은 해당 계층의 유닛 수에 따라 결정됩니다. 그리고 이 계산을 위해 필요한 W의 차원은 nL x nL-1 입니다. 이 규칙을 통해 각 W의 차원을 결정할 수 있습니다.\n",
    "\n",
    "Bias(편향) b는 각 계층의 활성화 값을 조정하는 역할을 합니다. 각 계층의 b는 해당 계층의 유닛 수와 동일한 차원을 갖습니다.\n",
    "\n",
    "백터화를 사용하면 여러 데이터 예제를 동시에 처리할 수 있습니다. 이 때 Z와 X의 차원은 데이터의 수 m만큼 확장되어 nL x m이 됩니다. 그러나 W와 b의 차원은 변경되지 않습니다.\n",
    "\n",
    "요약하면, 심층 신경망을 구현할 때 각 행렬과 벡터의 차원을 올바르게 설정하는 것은 중요하며, 이를 통해 코드에서 발생할 수 있는 오류를 줄일 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83858201",
   "metadata": {},
   "source": [
    "# Why Deep Representations?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb5d827",
   "metadata": {},
   "source": [
    "1. **계층적 표현**: 깊은 신경망은 계층적이며 구성적인 방식으로 데이터를 표현합니다.\n",
    "   - 예: 이미지에서 첫 번째 레이어는 간단한 특징(예: 에지)을 감지하고, 다음 레이어는 이러한 에지를 조합하여 더 복잡한 특징(예: 눈, 코)을 감지하며, 마지막 레이어는 얼굴과 같은 고차원의 특징을 감지합니다.\n",
    "\n",
    "2. **복잡한 함수**: 초기 레이어는 단순한 함수를 계산하는 반면, 깊은 레이어는 그러한 단순한 함수들을 조합하여 매우 복잡한 함수를 계산할 수 있습니다.\n",
    "\n",
    "3. **예: 음성 인식**: 깊은 신경망은 먼저 기본 오디오 파형 특징을 감지한 다음, 이를 조합하여 단어나 문장을 감지합니다.\n",
    "\n",
    "4. **심층 네트워크의 장점**: 회로 이론에 따르면, 일부 함수는 깊은 네트워크를 사용하면 작은 크기로 계산될 수 있지만, 얕은 네트워크를 사용하면 동일한 함수를 계산하기 위해 훨씬 더 많은 뉴런이 필요합니다.\n",
    "\n",
    "5. **깊은 학습의 인기**: 깊은 학습이란 용어는 '깊다'는 감각적인 면에서 상당히 인기를 얻었습니다. 그러나 실제로 깊은 신경망이 잘 작동하기 때문에 인기가 있습니다.\n",
    "\n",
    "6. **적절한 깊이 선택**: 항상 많은 수의 레이어가 필요한 것은 아닙니다. 문제에 따라 적절한 깊이를 선택하는 것이 중요합니다.\n",
    "\n",
    "요약하면, 깊은 신경망은 계층적이고 구성적인 방식으로 데이터를 처리하여 복잡한 패턴과 함수를 학습할 수 있습니다. 이러한 접근 방식은 다양한 응용 분야에서 높은 성능을 보여줍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a6340e",
   "metadata": {},
   "source": [
    "# Building Blocks of Deep Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b8e30378",
   "metadata": {},
   "source": [
    "1. **전방향 전파 (Forward Propagation)**\n",
    "    - 신경망의 각 레이어에서, 입력 값(활성화 값) $(a_{l-1}$)과 이전 레이어의 가중치 $(w_l$) 및 편향 $(b_l$)을 사용하여 다음 레이어의 활성화 값 $(a_l$)을 계산합니다.\n",
    "    - 계산 과정: $(z_l = w_l \\times a_{l-1} + b_l$) 그리고 $(a_l = g(z_l)$) 여기서 $(g$)는 활성화 함수입니다.\n",
    "    - $(z_l$) 값은 역전파에서 사용되기 때문에 캐시에 저장되어야 합니다.\n",
    "\n",
    "2. **역방향 전파 (Backward Propagation)**\n",
    "    - 이 과정은 전방향 전파의 반대로 수행됩니다. 입력으로 $(da(l)$)를 받아서 $(da(l-1)$)를 출력합니다.\n",
    "    - 그 과정에서 가중치와 편향에 대한 기울기 (gradient), 즉 $(dw_l$)와 $(db_l$)도 계산됩니다.\n",
    "\n",
    "3. **신경망의 기본 계산**\n",
    "    - 입력 특징 $(a_0$)을 가져와 첫 번째 레이어의 활성화 값을 계산합니다. 그리고 이것을 다음 레이어에 전달하며, 이런 과정을 반복하면서 마지막 레이어의 출력 값을 $(y_{hat}$)라고 합니다.\n",
    "    - $(y_{hat}$)을 계산한 후, 역방향 전파를 시작하게 됩니다. 이를 통해 모든 파라미터에 대한 기울기를 계산할 수 있습니다.\n",
    "\n",
    "4. **기울기 갱신**\n",
    "    - 역전파를 통해 얻은 기울기를 사용하여 신경망의 파라미터를 갱신합니다. 각 레이어의 가중치와 편향은 아래와 같이 갱신됩니다.\n",
    "      $(w_l = w_l - \\text{학습률} \\times dw_l$)\n",
    "      $(b_l = b_l - \\text{학습률} \\times db_l$)\n",
    "\n",
    "5. **캐시의 구현 세부 정보**\n",
    "    - 실제 구현 시, 캐시는 역전파를 위한 $(z_l$) 값 외에도 $(w_l$) 및 $(b_l$) 값도 저장하게 됩니다. \n",
    "\n",
    "결론적으로, 각 레이어마다 전방향 전파와 역방향 전파 단계가 있으며, 캐시를 사용하여 정보를 전달합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523e2577",
   "metadata": {},
   "source": [
    "# Forward and Backward Propagation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "89ee9a6b",
   "metadata": {},
   "source": [
    "### 전방향 전파(Forward Propagation)\n",
    "\n",
    "1. 입력 $(a^{l-1}$)과 가중치 $(w^l$), 편향 $(b^l$)을 사용해 $(z^l$)과 $(a^l$)을 계산합니다.\n",
    "2. $(z^l = w^l \\times a^{l-1} + b^l$)\n",
    "3. $(a^l = g(z^l)$) (여기서 $(g$)는 활성화 함수)\n",
    "4. $(z^l$)과 $(w^l$), $(b^l$)은 나중에 역방향 전파에서 사용하기 위해 캐시에 저장됩니다.\n",
    "\n",
    "### 역방향 전파(Backward Propagation)\n",
    "\n",
    "1. 입력으로 $(da^l$)을 받고, 출력으로 $(da^{l-1}$), $(dw^l$), $(db^l$)을 계산합니다.\n",
    "2. $(dz^l = da^l \\odot g'(z^l)$) (여기서 $(\\odot$)은 원소별 곱셈)\n",
    "3. $(dw^l = dz^l \\times a^{l-1}$)\n",
    "4. $(db^l = dz^l$)\n",
    "5. $(da^{l-1} = w^{lT} \\times dz^l$)\n",
    "\n",
    "위의 공식들을 코드로 옮기면, 각 레이어의 가중치와 편향을 업데이트하는 데 필요한 미분값(gradient)을 계산할 수 있습니다. 이런 계산은 일반적으로 훈련 데이터 전체에 대해 벡터화하여 수행됩니다.\n",
    "\n",
    "### 로직의 흐름\n",
    "\n",
    "1. 입력 $(x$) (또는 $(a^0$))을 받아 첫 번째 레이어부터 시작하여 전방향 전파를 수행합니다.\n",
    "2. 마지막 레이어에서 예측값 $(y$)-햇(y-hat)을 얻고, 이를 사용해 손실 함수의 값을 계산합니다.\n",
    "3. 손실 함수를 기반으로 역방향 전파를 시작하여 각 레이어의 가중치와 편향에 대한 미분값을 계산합니다.\n",
    "4. 이렇게 계산된 미분값을 사용하여 실제 가중치와 편향을 업데이트합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc0f8a1",
   "metadata": {},
   "source": [
    "# Parameters vs Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82e6a18",
   "metadata": {},
   "source": [
    "하이퍼 파라미터는 딥 러닝 모델을 학습시키기 위해 조절해야 하는 값들을 말합니다. 모델의 파라미터는 W(가중치)와 B(편향)이며, 이들은 학습 과정을 통해 최적화됩니다. 반면 하이퍼 파라미터는 학습 과정 자체를 제어하며, 학습 속도, 수렴 속도 등에 큰 영향을 미칩니다.\n",
    "\n",
    "주요 하이퍼 파라미터 예시:\n",
    "- 학습률(α): 파라미터 업데이트 속도를 결정\n",
    "- 반복 횟수: 경사 하강법의 반복 횟수\n",
    "- 은닉층의 수(L)\n",
    "- 은닉 유닛의 수\n",
    "- 활성화 함수 선택: RELU, tanh, 시그모이드 등\n",
    "\n",
    "하이퍼 파라미터는 모델의 성능에 결정적인 영향을 미치므로, 적절한 값을 찾는 것이 중요합니다. 그러나 최적의 하이퍼 파라미터 값을 미리 알아낼 수 있는 정확한 방법은 없으므로, 다양한 값을 실험적으로 시도해보는 과정이 필요합니다.\n",
    "\n",
    "딥 러닝의 발전에 따라 하이퍼 파라미터의 종류가 늘어났기 때문에, 기존의 기계 학습에서는 α 같은 값들을 단순히 '파라미터'라고 부르기도 했지만, 현재는 이를 '하이퍼 파라미터'라고 구분하여 부릅니다.\n",
    "\n",
    "결론적으로, 딥 러닝을 적용하는 것은 많은 시행 착오를 통해 최적의 하이퍼 파라미터 값을 찾는 과정이 필요하며, 이는 경험과 실험을 바탕으로 이루어집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d54b70c",
   "metadata": {},
   "source": [
    "# What does this have to do with the brain?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e180ac",
   "metadata": {},
   "source": [
    "딥 러닝과 두뇌 사이에는 어느 정도 비슷한 점이 있습니다. 하지만 이 두 개념을 직접적으로 연결하는 것은 너무 과장된 해석일 수 있습니다. \n",
    "\n",
    "1. **딥 러닝과 뇌의 연관성**: 딥 러닝의 신경망과 두뇌의 신경 세포 사이에는 어느 정도 비슷한 점이 있습니다. 신경망의 뉴런이 일정 임계값을 넘으면 활성화되는 것과, 뇌의 신경 세포가 특정 자극에 반응하여 전기적 신호를 보내는 것 사이에는 유사점이 있습니다.\n",
    "\n",
    "2. **뇌의 복잡성**: 그러나 실제 뇌의 신경 세포는 매우 복잡하며, 하나의 신경 세포가 어떻게 작동하는지에 대한 정확한 이해는 아직 부족합니다. 뇌가 어떻게 학습하는지에 대한 과정 또한 아직 미스터리입니다. 딥 러닝의 역전파나 경사 하강법 같은 알고리즘이 뇌에 존재하는지 여부도 아직 확실하지 않습니다.\n",
    "\n",
    "3. **딥 러닝의 본질**: 딥 러닝은 복잡한 함수를 학습하는 데 매우 효과적입니다. 입력과 출력 사이의 관계를 학습하는 데 중점을 둡니다. 반면, 뇌와의 유사성은 초기에는 설명을 위한 비유로 사용되었을 수 있지만, 현재는 그렇게 유용하지 않을 수 있습니다.\n",
    "\n",
    "4. **비전에 대한 영감**: 컴퓨터 비전 분야는 뇌에서 더 많은 영감을 받았을 수 있습니다. 하지만 개인적으로, 저자는 딥 러닝을 뇌와 연결하는 비유를 이전보다 덜 사용하게 되었습니다.\n",
    "\n",
    "요약하면, 딥 러닝과 뇌 사이에는 어느 정도 유사점이 있지만, 이 둘을 직접적으로 연결하는 것은 부정확할 수 있습니다. 딥 러닝은 복잡한 함수를 학습하는 도구로, 뇌의 작동 원리를 완전히 반영하는 것은 아닙니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2535bdec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

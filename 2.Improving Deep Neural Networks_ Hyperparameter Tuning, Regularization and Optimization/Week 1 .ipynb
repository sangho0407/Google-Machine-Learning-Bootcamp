{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6b41552",
   "metadata": {},
   "source": [
    "# Setting up your Machine Learning Application "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baabee5",
   "metadata": {},
   "source": [
    "### Train / Dev / Test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6f0094",
   "metadata": {},
   "source": [
    "1. **디프 러닝의 실용적인 측면**\n",
    "\n",
    "    - 딥 러닝에서는 많은 결정들을 내려야 합니다: 신경망의 계층 수, 각 계층의 숨겨진 단위의 수, 학습 속도, 활성화 함수 등을 결정해야 합니다.\n",
    "    - 초기 시도에서 모든 것을 올바르게 선택하는 것은 거의 불가능합니다. 그렇기 때문에 응용 기계 학습은 반복적인 프로세스입니다.\n",
    "  \n",
    "2. **현대 딥 러닝의 성공**\n",
    "    - 딥 러닝은 자연어 처리, 컴퓨터 비전, 음성 인식, 구조화된 데이터에서 큰 성공을 거두었습니다.\n",
    "    - 다양한 분야에서의 경험이 다른 분야로 그대로 전환되지 않을 수 있습니다. 따라서 적절한 하이퍼파라미터를 처음부터 정확하게 추측하는 것은 거의 불가능합니다.\n",
    "  \n",
    "3. **데이터 세트 설정**\n",
    "    - 전통적으로 사용 가능한 모든 데이터를 훈련, 검증(또는 개발), 테스트 세트로 나눕니다.\n",
    "    - 큰 데이터 세트를 가지고 있다면, 검증 및 테스트 세트의 크기를 줄일 수 있습니다. \n",
    "\n",
    "4. **불일치된 훈련 및 테스트 분포**\n",
    "    - 딥 러닝에서는 훈련 데이터와 테스트 데이터가 다른 분포를 가질 수 있습니다.\n",
    "    - 중요한 것은 개발 세트와 테스트 세트가 같은 분포를 가져야 한다는 것입니다.\n",
    "\n",
    "5. **테스트 세트의 필요성**\n",
    "    - 모든 상황에서 테스트 세트가 필요한 것은 아닙니다. \n",
    "    - 테스트 세트의 목적은 최종 모델의 성능에 대한 편향되지 않은 추정을 제공하는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f31064",
   "metadata": {},
   "source": [
    "### VideoBias / Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66f713e",
   "metadata": {},
   "source": [
    "1. **바이어스(Bias)와 분산(Variance)의 이해**:\n",
    "   - 바이어스는 모델이 학습 데이터에 대해서 얼마나 잘 일반화되지 않는지를 나타냅니다. 높은 바이어스를 가진 모델은 데이터의 패턴을 충분히 학습하지 못해 과소적합(underfitting)이 발생합니다.\n",
    "   - 분산은 모델이 학습 데이터의 작은 변화에 얼마나 민감한지를 나타냅니다. 높은 분산을 가진 모델은 학습 데이터의 노이즈까지 너무 잘 학습해버려 과대적합(overfitting)이 발생합니다.\n",
    "\n",
    "2. **딥러닝에서의 바이어스/분산 트레이드오프**:\n",
    "   - 과거에는 바이어스와 분산 사이에는 트레이드오프 관계가 있다고 여겨졌습니다. 즉, 바이어스를 줄이면 분산이 증가하고, 분산을 줄이면 바이어스가 증가하는 경향이 있었습니다.\n",
    "   - 그러나 딥러닝에서는 이런 트레이드오프가 크게 관찰되지 않습니다. 복잡한 네트워크와 충분한 데이터를 이용하면 동시에 바이어스와 분산을 줄일 수 있습니다.\n",
    "\n",
    "3. **학습 오차와 개발 오차**:\n",
    "   - 학습 오차와 개발 오차를 비교함으로써 바이어스와 분산의 문제를 진단할 수 있습니다.\n",
    "   - 학습 오차가 높으면 바이어스가 높은 것이고, 학습 오차에 비해 개발 오차가 훨씬 높으면 분산이 높은 것입니다.\n",
    "\n",
    "4. **베이즈 오차**:\n",
    "   - 최적의 분류기로도 달성할 수 없는 최소의 오차를 말합니다. 이 값이 높은 경우, 바이어스와 분산을 진단하는 방법이 다를 수 있습니다.\n",
    "\n",
    "요약하자면, 바이어스와 분산은 머신러닝 모델의 성능을 평가하고 문제점을 진단하는 데 중요한 요소입니다. 이를 통해 모델이 학습 데이터에 과소적합하거나 과대적합하는지 알 수 있습니다. 따라서 바이어스와 분산의 개념을 잘 이해하는 것은 머신러닝의 성능을 향상시키는 데 도움이 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ae45da",
   "metadata": {},
   "source": [
    "### Basic Recipe for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c09aa1f",
   "metadata": {},
   "source": [
    "머신 러닝 모델을 학습할 때, 우리는 주로 두 가지 큰 문제, 즉 **편향(bias)**과 **분산(variance)**을 직면합니다.\n",
    "\n",
    "1. **편향 문제(Bias Problem)**\n",
    "   - 편향이 높다는 것은 모델이 훈련 데이터를 잘 학습하지 못한다는 것을 의미합니다.\n",
    "   - 해결책:\n",
    "     - 더 큰 네트워크(더 많은 히든 레이어나 유닛) 사용\n",
    "     - 더 오래 학습\n",
    "     - 다른 최적화 알고리즘 사용\n",
    "     - 적합한 신경망 구조 찾기(이것은 실험적으로 접근해야 할 수 있음)\n",
    "\n",
    "2. **분산 문제(Variance Problem)**\n",
    "   - 분산이 높다는 것은 모델이 훈련 데이터에는 잘 맞지만 새로운 데이터(개발 세트)에는 잘 일반화하지 못한다는 것을 의미합니다.\n",
    "   - 해결책:\n",
    "     - 더 많은 훈련 데이터 수집\n",
    "     - 정규화(regularization) 사용\n",
    "     - 적합한 신경망 구조 찾기\n",
    "\n",
    "과거에는 편향과 분산 사이에 균형을 잡아야 하는 \"편향-분산 트레이드오프\"라는 개념이 있었습니다. 그러나 현대 딥 러닝에서는 큰 네트워크를 학습시키거나 더 많은 데이터를 얻는 방법으로 편향만 줄이거나 분산만 줄이는 방법을 갖게 되었습니다.\n",
    "\n",
    "따라서, 주어진 문제에 따라 편향 또는 분산 문제를 진단하고 적절한 접근 방식을 선택하는 것이 중요합니다. \n",
    "\n",
    "정규화는 네트워크가 과적합(overfitting)되는 것을 방지하는 데 유용한 기법입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01c7bf6",
   "metadata": {},
   "source": [
    "# Regularizing your Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf38a54",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63fcc7b",
   "metadata": {},
   "source": [
    "신경망이 데이터에 과적합(overfitting)되는 것을 방지하기 위한 일반적인 방법 중 하나는 정규화(regularization)입니다. 정규화는 모델의 복잡성을 제한하여 훈련 데이터에 너무 딱 맞게 학습되는 것을 막습니다.\n",
    "\n",
    "**로지스틱 회귀에서의 정규화:**\n",
    "로지스틱 회귀에서의 비용 함수 $( J $)를 최소화하려고 합니다. 정규화를 추가하려면 비용 함수에 $( \\frac{\\lambda}{2m} \\times ||w||^2 $) 항을 추가합니다. 여기서 $( $lambda $)는 정규화 파라미터로, 너무 큰 값을 갖는 가중치 $(w)를 제한하기 위해 사용됩니다. \n",
    "\n",
    "$( ||w||^2 $)는 L2 노름이라고도 불리며, 이는 가중치의 각 요소를 제곱한 값의 합입니다. 이 정규화 방법은 L2 정규화라고도 합니다. L1 정규화의 경우 가중치의 절대값을 사용합니다.\n",
    "\n",
    "**신경망에서의 정규화:**\n",
    "신경망에서는 여러 층의 가중치 $( w[1], w[2], ... $)가 있습니다. 정규화를 추가하려면 모든 가중치 행렬에 대한 제곱합을 비용 함수에 추가합니다. 이 값은 프로베니우스 노름(Frobenius norm)으로 알려져 있으며, 행렬의 모든 요소의 제곱의 합입니다.\n",
    "\n",
    "정규화를 적용한 후 경사 하강법을 사용하여 가중치를 업데이트할 때 $( w $)에 대한 업데이트는 $( dw +frac{\\lambda}{m} \\times w $)에 비례하는 값으로 변경됩니다.\n",
    "\n",
    "이러한 정규화 방법을 사용하면 가중치가 감소하는 경향이 있습니다. 따라서 L2 정규화는 종종 'weight decay'라고도 불립니다.\n",
    "\n",
    "**왜 정규화는 과적합을 방지하는가?**\n",
    "정규화는 모델의 가중치를 제한하여 모델이 훈련 데이터에 과도하게 적응하는 것을 방지합니다. 가중치가 작아지면 모델의 복잡성이 감소하게 되므로 일반화 능력이 향상됩니다.\n",
    "\n",
    "마지막으로, $( $lambda $)는 하이퍼파라미터로서 개발 세트나 교차 검증을 통해 조정해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972e18ce",
   "metadata": {},
   "source": [
    "### Why Regularization Reduces Overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafb5c13",
   "metadata": {},
   "source": [
    "정규화는 기본적으로 모델이 학습 데이터에 너무 맞춰져 있어서 새로운 데이터에 대한 성능이 떨어지는 과적합 문제를 해결하기 위한 방법 중 하나입니다.\n",
    "\n",
    "1. **과적합의 예시**\n",
    "   - 고분산과 고편향: 고분산은 모델이 학습 데이터에 지나치게 적응하여 복잡한 경계선을 그리는 것을 의미하며, 고편향은 모델이 너무 단순해서 데이터의 패턴을 잡아내지 못하는 것을 의미합니다.\n",
    "   \n",
    "2. **정규화의 동작 원리**\n",
    "   - 정규화는 비용 함수에 패널티 항을 추가하여 모델의 가중치가 너무 큰 값을 가지지 않도록 합니다.\n",
    "   - L2 정규화는 Frobenius norm(프로베니우스 노름)을 사용하여 가중치의 크기를 패널티로 부과합니다.\n",
    "   \n",
    "3. **정규화가 과적합을 방지하는 직관적 이해**\n",
    "   - 큰 정규화 값 (λ)을 사용하면 가중치가 작아집니다. 따라서 신경망의 복잡성이 줄어들어 데이터에 과도하게 적응하는 것을 방지할 수 있습니다.\n",
    "   - 실제로 모든 은닉 유닛이 0으로 설정되는 것은 아니지만, 각 은닉 유닛의 효과가 작아져 신경망이 단순화됩니다.\n",
    "   \n",
    "4. **tan h 활성화 함수와의 관계**\n",
    "   - tan h 함수는 z의 값이 작을 때 선형과 유사합니다. 따라서 z의 값이 작으면 신경망은 선형적인 동작을 합니다.\n",
    "   - 큰 정규화 값 때문에 가중치와 z가 작아질 때, 신경망은 복잡한 비선형 경계선 대신 단순한 선형 경계선을 갖게 됩니다.\n",
    "   \n",
    "5. **구현 팁**\n",
    "   - 정규화를 사용할 때 비용 함수는 기존의 정의에서 추가된 패널티 항을 포함하게 됩니다.\n",
    "   - 그래디언트 디센트를 디버깅할 때는 이 새로운 비용 함수 정의를 사용해야 합니다.\n",
    "\n",
    "정리하면, 정규화는 모델의 가중치가 너무 큰 값을 가지는 것을 방지하여 모델이 학습 데이터에 과도하게 적응하는 것을 방지하며, 이를 통해 새로운 데이터에 대한 모델의 성능을 향상시키는 효과가 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92d2058",
   "metadata": {},
   "source": [
    "### Dropout Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3d0053",
   "metadata": {},
   "source": [
    "1. **Dropout의 기본 아이디어**:\n",
    "   - Dropout은 학습 과정 중에 신경망의 뉴런(또는 노드)을 임의로 꺼서(즉, 0으로 설정하여) 실제로 학습에 사용되지 않게 하는 기법입니다. 이렇게 하면 신경망이 특정 뉴런에 너무 의존하지 않게 되어 과적합을 줄일 수 있습니다.\n",
    "\n",
    "2. **실제 구현**:\n",
    "   - 예를 들어, 학습 중에 각 뉴런을 유지할 확률을 0.8로 설정하면, 각 뉴런은 80%의 확률로 유지되고 20%의 확률로 꺼집니다. 이를 구현하기 위해 각 뉴런에 대해 임의의 숫자를 생성하고 그 숫자가 0.8보다 작은지 확인하여 Dropout을 결정합니다.\n",
    "   - 이렇게 하면, 각 학습 단계에서 신경망의 서로 다른 부분이 꺼지게 되므로, 신경망은 데이터의 다양한 특징을 학습하게 됩니다.\n",
    "\n",
    "3. **Inverted Dropout**:\n",
    "   - Dropout을 사용하면 실제로 일부 뉴런이 비활성화되기 때문에, 활성화된 뉴런의 출력 값의 평균이 변경될 수 있습니다. 이를 보정하기 위해 \"Inverted Dropout\" 기법을 사용하여 활성화된 뉴런의 출력 값을 적절한 확률로 나눠줍니다.\n",
    "   - 예를 들어, 유지 확률이 0.8인 경우 출력 값을 0.8로 나누어 기대 출력 값을 일정하게 유지합니다.\n",
    "\n",
    "4. **테스트 시에는**:\n",
    "   - 모델을 실제로 사용할 때 (즉, 테스트나 실제 운영 환경에서) Dropout을 사용하지 않습니다. 왜냐하면 예측 시에는 모든 뉴런을 사용하여 가장 효과적인 성능을 얻기 위함입니다.\n",
    "\n",
    "Dropout의 이점은 모델이 훈련 데이터에 너무 특화되지 않게 하여 일반화 성능을 향상시키는 것입니다. 이는 딥 러닝 모델이 복잡하고 많은 수의 파라미터를 가질 때 특히 중요합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce9d85d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85f648a4",
   "metadata": {},
   "source": [
    "### Understanding Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f40774",
   "metadata": {},
   "source": [
    "**드롭아웃(Dropout)이란?**\n",
    "드롭아웃은 신경망 훈련 과정에서 일부 뉴런(유닛)을 임의로 꺼주는 기법입니다. 이 과정에서 신경망의 각 뉴런은 다른 특정 뉴런의 존재에 너무 의존적이지 않게 되며, 이로 인해 신경망의 과대적합(overfitting)을 방지하는 효과가 있습니다.\n",
    "\n",
    "**왜 드롭아웃이 잘 작동하는가?**\n",
    "1. **일종의 앙상블 학습**: 드롭아웃은 매번 다른 서브넷(subnet)을 사용하여 훈련시키는 것으로 볼 수 있습니다. 이렇게 다양한 작은 신경망들을 훈련시키는 것은 앙상블 학습의 일종과 유사하며, 일반화 능력을 향상시킵니다.\n",
    "\n",
    "2. **뉴런 간의 의존성 감소**: 뉴런의 입력 중 일부가 임의로 사라질 수 있기 때문에, 각 뉴런은 다른 특정 뉴런의 활성화에 의존하는 것을 피하게 됩니다. 이로 인해 가중치가 균형있게 조정되며, 이는 과대적합을 방지합니다.\n",
    "\n",
    "드롭아웃은 L2 정규화와 유사한 효과를 가지지만, L2 정규화와는 달리 가중치에 적용되는 정규화 정도가 다를 수 있습니다.\n",
    "\n",
    "**드롭아웃의 구현과 관련된 팁**:\n",
    "1. **계층별 Keep-prob**: 'keep-prob'은 드롭아웃 과정에서 뉴런을 유지할 확률을 의미합니다. 드롭아웃을 적용하는 각 계층마다 다른 'keep-prob' 값을 설정할 수 있습니다.\n",
    "\n",
    "2. **컴퓨터 비전과 드롭아웃**: 컴퓨터 비전 분야에서는 입력 데이터의 크기가 크기 때문에 과대적합 문제가 자주 발생합니다. 따라서 이 분야에서는 드롭아웃이 널리 사용됩니다.\n",
    "\n",
    "3. **비용 함수의 문제**: 드롭아웃을 사용하면 비용 함수 J의 값이 명확히 정의되지 않습니다. 이로 인해 훈련 과정을 시각적으로 모니터링하기 어려울 수 있습니다.\n",
    "\n",
    "결론적으로, 드롭아웃은 신경망이 과대적합하는 것을 방지하는 강력한 정규화 기법 중 하나입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff60c69",
   "metadata": {},
   "source": [
    "# Setting Up your Optimization Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00810b72",
   "metadata": {},
   "source": [
    "### Normalizing Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0eed8c",
   "metadata": {},
   "source": [
    "1. **정규화의 과정**:\n",
    "    - **평균 빼기**: 먼저, 트레이닝 세트의 모든 입력 값에서 평균을 빼서 평균이 0이 되게 합니다.\n",
    "    - **분산 정규화**: 다음으로, 각 입력 특성의 분산을 1로 조정합니다. 이는 각 입력 값을 해당 특성의 표준 편차로 나눔으로써 이루어집니다.\n",
    "    \n",
    "2. **테스트 세트에 대한 주의사항**: 훈련 데이터를 정규화할 때 사용한 동일한 평균과 표준 편차를 사용하여 테스트 데이터도 정규화해야 합니다. \n",
    "\n",
    "3. **왜 정규화가 중요한가?**: \n",
    "    - **비효율적인 최적화**: 입력 특성들이 다른 스케일을 가질 때, 비용 함수는 길쭉한 형태를 가지게 될 수 있습니다. 이런 형태의 비용 함수는 경사 하강법으로 최적화하는데 비효율적입니다. \n",
    "    - **효율적인 최적화**: 반면, 입력 특성들이 유사한 스케일을 가지면 비용 함수는 더 원형에 가까운 형태를 가집니다. 이런 형태의 비용 함수는 경사 하강법으로 더 빠르게 최적화할 수 있습니다.\n",
    "    - **훈련 속도 개선**: 정규화는 신경망의 훈련 속도를 크게 개선할 수 있습니다. 입력 특성들이 크게 다른 스케일을 가질 때 이 효과는 특히 두드러집니다.\n",
    "\n",
    "4. **일반적인 권장사항**: 입력 특성들이 비슷한 스케일을 가지고 있다면 정규화는 그렇게 중요하지 않을 수 있습니다. 그러나 대부분의 경우 정규화는 학습 속도를 향상시키며, 나쁜 영향을 미치는 경우는 거의 없습니다. \n",
    "\n",
    "요약하면, 정규화는 신경망의 훈련을 가속화하고, 최적화 과정을 개선하기 위해 사용되는 기술입니다. 입력 특성들이 다양한 범위나 스케일을 가질 때 특히 중요합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789f8d74",
   "metadata": {},
   "source": [
    "### Vanishing / Exploding Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3f4863",
   "metadata": {},
   "source": [
    "딥 뉴럴 네트워크, 특히 매우 깊은 네트워크를 훈련할 때 발생하는 문제 중 하나는 기울기가 폭발하거나 소실되는 것입니다. 이것은 깊은 네트워크를 훈련할 때 도함수나 기울기가 매우 크거나 매우 작아질 수 있음을 의미하며, 때로는 지수적으로 작아질 수 있습니다. 이로 인해 훈련이 어려워집니다.\n",
    "\n",
    "간단히 예를 들어, 우리가 선형 활성화 함수를 사용하고 편향(bias)을 무시한다고 가정해봅시다. 그럼 출력 Y는 W 행렬들의 연속된 곱과 입력 X의 곱으로 표현될 수 있습니다. \n",
    "\n",
    "만약 각 가중치 행렬이 단위 행렬보다 약간 큰 값, 예를 들어 1.5의 단위 행렬로 설정된다면, Y의 값은 지수적으로 크게 증가할 것입니다. 반대로, 각 행렬이 1보다 작은 값, 예를 들어 0.5의 단위 행렬로 설정되면, Y의 값은 지수적으로 감소합니다.\n",
    "\n",
    "이러한 통찰을 통해, 가중치 W가 단위 행렬보다 약간 크면 활성화는 폭발할 것이고, 가중치가 단위 행렬보다 약간 작으면 활성화는 지수적으로 감소한다는 것을 알 수 있습니다.\n",
    "\n",
    "딥 뉴럴 네트워크에서는 이러한 활성화 값이나 기울기 값이 지수적으로 증가하거나 감소할 수 있으므로, 이 문제를 해결하기 위한 방법이 필요합니다. 이 문제의 부분적인 해결책은 가중치 초기화를 신중하게 선택하는 것입니다. 신중한 가중치 초기화는 기울기의 폭발이나 소실 문제를 완전히 해결하지는 않지만 크게 도움을 줄 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737b6dec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

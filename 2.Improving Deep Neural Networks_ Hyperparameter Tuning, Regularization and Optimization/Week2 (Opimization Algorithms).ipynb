{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "142edce1",
   "metadata": {},
   "source": [
    "### Mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27774604",
   "metadata": {},
   "source": [
    "큰 데이터셋에서 딥러닝을 훈련시킬 때, 전체 데이터셋을 한 번에 처리하기보다는 일부분씩 나누어 처리하는 것이 효율적일 수 있습니다. 이렇게 나눈 작은 데이터 묶음을 \"미니 배치\"라고 부릅니다.\n",
    "\n",
    "**미니 배치 경사하강법(Mini-batch Gradient Descent)**은 큰 데이터셋을 여러 개의 작은 '미니 배치'로 나누어 경사하강법을 적용하는 방법입니다.\n",
    "\n",
    "1. **미니 배치의 이해**: 만약 우리의 전체 트레이닝 셋에 5백만 개의 샘플이 있다면, 이것을 한 번에 처리하는 것은 매우 느릴 수 있습니다. 그래서 이를 예를 들면, 1,000개의 샘플로 나누어서 5,000개의 미니 배치로 만들 수 있습니다.\n",
    "\n",
    "2. **미니 배치 경사하강법의 동작**: 전체 데이터셋을 사용하여 한 번의 경사하강법 스텝을 수행하는 대신, 미니 배치를 사용하여 여러 번의 경사하강법 스텝을 수행합니다. 이 과정에서 각 미니 배치에 대해 순전파(forward propagation)와 역전파(back propagation)를 수행하여 가중치를 업데이트합니다.\n",
    "\n",
    "3. **에포크(Epoch)**: 전체 트레이닝 데이터셋을 한 번 통과하는 것을 에포크라고 합니다. 미니 배치 경사하강법에서는 한 번의 에포크 동안 여러 개의 경사하강법 스텝을 수행합니다.\n",
    "\n",
    "미니 배치 경사하강법의 장점은 큰 데이터셋에 대해 훨씬 빠른 수렴을 가능하게 합니다. 전체 데이터셋에 대해 경사하강법을 수행하는 것보다 각 미니 배치에 대해 여러 번의 업데이트를 수행함으로써 빠른 피드백을 받고, 가중치를 효율적으로 업데이트할 수 있습니다.\n",
    "\n",
    "단, 미니 배치 경사하강법의 수렴은 때로는 불안정 할 수 있으며, 좋은 미니 배치 크기나 학습률을 선택하는 것이 중요합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6c155e",
   "metadata": {},
   "source": [
    "### Understanding Mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef244dd",
   "metadata": {},
   "source": [
    "### 배치 경사 하강법 vs 미니-배치 경사 하강법\n",
    "* **배치 경사 하강법**: 모든 학습 데이터를 사용하여 한 번에 경사 하강법을 실행합니다. 이 방법은 각 반복마다 비용 함수가 감소하므로 안정적입니다. 그러나 대규모 데이터셋에서는 한 단계를 완료하는 데 시간이 오래 걸릴 수 있습니다.\n",
    "  \n",
    "* **미니-배치 경사 하강법**: 전체 데이터셋을 여러 미니-배치로 분할하여 각 미니-배치를 사용하여 경사 하강법을 실행합니다. 이는 전체 데이터셋을 처리하기 전에도 업데이트를 할 수 있게 해주므로, 학습 속도가 빠를 수 있습니다.\n",
    "\n",
    "### 미니-배치의 비용 그래프\n",
    "미니-배치 경사 하강법을 사용하면 비용 그래프가 더 불안정할 수 있습니다. 각 미니-배치마다 데이터 분포가 약간 다르기 때문에, 비용 함수가 항상 감소하는 것은 아닙니다. 그럼에도 불구하고 전반적인 추세는 비용이 감소하는 방향으로 나타나야 합니다.\n",
    "\n",
    "### 미니-배치 크기 선택\n",
    "* **배치 크기 = m**: 전체 데이터셋 크기와 같으면, 이것은 단순히 배치 경사 하강법입니다.\n",
    "  \n",
    "* **배치 크기 = 1**: 각 샘플을 독립적으로 처리하면 확률적 경사 하강법(Stochastic Gradient Descent, SGD)이 됩니다. 이 방법은 매우 노이즈가 많을 수 있습니다.\n",
    "\n",
    "실제로 가장 좋은 미니-배치 크기는 1과 m 사이입니다. 너무 큰 배치 크기는 한 번의 반복이 너무 오래 걸리게 만들고, 너무 작은 배치 크기는 벡터화의 장점을 잃게 만듭니다. 일반적으로 미니-배치 크기는 64, 128, 256, 512 등 2의 제곱을 사용하는 경우가 많습니다.\n",
    "\n",
    "### 활용 팁\n",
    "* 데이터셋이 작은 경우(예: 2000개 미만): 미니-배치를 사용하는 대신 전체 배치 경사 하강법을 사용합니다.\n",
    "  \n",
    "* 미니-배치 크기 선택: 학습 세트의 크기, 메모리 사용량, 벡터화 속도 등을 고려하여 최적의 크기를 선택합니다. 때로는 여러 가지 크기를 실험하여 가장 효과적인 것을 찾아야 할 수도 있습니다.\n",
    "\n",
    "---\n",
    "\n",
    "미니-배치 경사 하강법은 대규모 데이터셋에서 효율적인 학습을 가능하게 합니다. 하지만 그 자체로도 최적화할 수 있는 다양한 요소가 있으며, 다른 고급 최적화 기법들과 함께 사용될 때 더욱 효과적일 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002c95f8",
   "metadata": {},
   "source": [
    "### Exponentially Weighted Averages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5010b73c",
   "metadata": {},
   "source": [
    "\n",
    "### **지수 가중 평균 (Exponentially Weighted Averages)**\n",
    "1. **메모리 효율성**: 지수 가중 평균은 이전의 모든 데이터를 저장할 필요 없이 이전 평균만을 유지하면서 계산됩니다.\n",
    "2. **반응성**: $( \\beta $) 값에 따라 최근의 변화에 얼마나 빠르게 반응할 것인지 조절할 수 있습니다.\n",
    "3. **부드러운 곡선**: 데이터에 있는 노이즈나 진동을 줄여서 부드러운 곡선을 생성합니다.\n",
    "\n",
    "### **장점**\n",
    "1. **단순성**: 구현하기가 간단하며 연산량이 적습니다.\n",
    "2. **효율성**: 큰 데이터 세트에서도 잘 동작하며 메모리 효율이 좋습니다.\n",
    "3. **유연성**: $( \\beta $) 값을 조절하여 다양한 시나리오에 적응시킬 수 있습니다.\n",
    "\n",
    "### **동작 원리**\n",
    "지수 가중 평균은 이름에서 알 수 있듯이 가중치가 지수적으로 감소하는 방식으로 평균을 계산합니다. 이는 가장 최근의 데이터가 평균에 큰 영향을 미치고, 오래된 데이터는 점차 작은 영향을 미치도록 설계되었습니다.\n",
    "\n",
    "1. **초기화**: $( V_0 $)를 0으로 설정합니다.\n",
    "2. **가중치 업데이트**: 매일 다음과 같은 식을 사용하여 가중치를 업데이트합니다.\n",
    "$[ V_t = \\beta \\times V_{t-1} + (1-\\beta) \\times \\theta_t $]\n",
    "여기서 $( \\theta_t $)는 해당 날짜의 데이터입니다.\n",
    "\n",
    "이러한 방식으로 가장 최근의 데이터에 더 큰 가중치를 부여하고, 과거의 데이터에는 점점 더 작은 가중치를 부여합니다. $( \\beta $) 값은 이러한 가중치의 감소 속도를 조절합니다.\n",
    "\n",
    "**예시**: 작년의 런던 일일 온도\n",
    "\n",
    "데이터를 이해하기 위한 배경: \n",
    "- 작성자는 현재 미국에 거주하고 있지만 런던에서 태어났습니다.\n",
    "- 그는 런던의 작년 일일 온도를 예시로 사용하였습니다. \n",
    "- 예를 들어, 1월 1일 온도는 4도였고, 1월 2일은 9도였다.\n",
    "- 1년의 중간, 즉 5월 말경에는 15도였다.\n",
    "\n",
    "이렇게 데이터를 그래프로 그리면 노이즈가 많이 보이는데, 이를 좀 더 부드럽게 만들기 위해서 지수 가중 평균을 사용할 수 있습니다.\n",
    "\n",
    "### 지수 가중 평균 계산\n",
    "\n",
    "1. $( V_0 $) 를 0으로 초기화합니다.\n",
    "2. 매일 지수 가중 평균을 다음과 같은 식으로 계산합니다: \n",
    "$[ V_t = \\beta \\times V_{t-1} + (1-\\beta) \\times \\theta_t $]\n",
    "여기서 $( \\theta_t $) 는 그날의 온도입니다.\n",
    "\n",
    "기본적으로 이 식은 오늘의 평균 온도와 이전 날의 평균 온도를 결합하여 새로운 평균 온도를 생성합니다.\n",
    "\n",
    "### β의 역할\n",
    "\n",
    "β 값을 조정하면 평균의 \"부드러움\"이나 \"반응성\"이 변경됩니다. \n",
    "\n",
    "- $( \\beta = 0.9 $) 일 때, 약 10일 동안의 온도를 평균낸 것과 비슷합니다.\n",
    "- $( \\beta = 0.98 $) 일 때, 약 50일 동안의 온도를 평균낸 것과 비슷합니다. 그 결과로 훨씬 부드러운 곡선이 만들어집니다. 그러나, 최근의 변화에 더 느리게 반응합니다.\n",
    "- $( \\beta = 0.5 $) 일 때, 오직 2일 동안의 온도만을 평균낸 것과 비슷하므로, 노이즈가 많이 보이지만 최근의 변화에는 빠르게 반응합니다.\n",
    "\n",
    "결론적으로, 지수 가중 평균은 데이터의 부드러운 평균을 생성하기 위한 방법입니다. $( \\beta $) 값에 따라 평균의 부드러움과 반응성을 조절할 수 있습니다. 이러한 지수 가중 평균 방식은 추후 소개될 고급 최적화 알고리즘에서 중요한 역할을 합니다.\n",
    "\n",
    "---\n",
    "\n",
    "지수 가중 평균은 많은 시계열 데이터 분석에서 유용하게 사용되며, 특히 머신러닝의 최적화 알고리즘에서 그 중요성이 높아집니다. $( \\beta $)의 선택은 데이터의 특성 및 응용 분야에 따라 달라질 수 있으므로, 다양한 값을 실험하여 최적의 결과를 얻는 것이 중요합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c61e5711",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

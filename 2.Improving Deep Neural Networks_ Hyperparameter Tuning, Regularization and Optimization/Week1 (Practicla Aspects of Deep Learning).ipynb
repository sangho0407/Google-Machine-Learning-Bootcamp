{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6b41552",
   "metadata": {},
   "source": [
    "# Setting up your Machine Learning Application "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baabee5",
   "metadata": {},
   "source": [
    "### Train / Dev / Test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6f0094",
   "metadata": {},
   "source": [
    "1. **디프 러닝의 실용적인 측면**\n",
    "\n",
    "    - 딥 러닝에서는 많은 결정들을 내려야 합니다: 신경망의 계층 수, 각 계층의 숨겨진 단위의 수, 학습 속도, 활성화 함수 등을 결정해야 합니다.\n",
    "    - 초기 시도에서 모든 것을 올바르게 선택하는 것은 거의 불가능합니다. 그렇기 때문에 응용 기계 학습은 반복적인 프로세스입니다.\n",
    "  \n",
    "2. **현대 딥 러닝의 성공**\n",
    "    - 딥 러닝은 자연어 처리, 컴퓨터 비전, 음성 인식, 구조화된 데이터에서 큰 성공을 거두었습니다.\n",
    "    - 다양한 분야에서의 경험이 다른 분야로 그대로 전환되지 않을 수 있습니다. 따라서 적절한 하이퍼파라미터를 처음부터 정확하게 추측하는 것은 거의 불가능합니다.\n",
    "  \n",
    "3. **데이터 세트 설정**\n",
    "    - 전통적으로 사용 가능한 모든 데이터를 훈련, 검증(또는 개발), 테스트 세트로 나눕니다.\n",
    "    - 큰 데이터 세트를 가지고 있다면, 검증 및 테스트 세트의 크기를 줄일 수 있습니다. \n",
    "\n",
    "4. **불일치된 훈련 및 테스트 분포**\n",
    "    - 딥 러닝에서는 훈련 데이터와 테스트 데이터가 다른 분포를 가질 수 있습니다.\n",
    "    - 중요한 것은 개발 세트와 테스트 세트가 같은 분포를 가져야 한다는 것입니다.\n",
    "\n",
    "5. **테스트 세트의 필요성**\n",
    "    - 모든 상황에서 테스트 세트가 필요한 것은 아닙니다. \n",
    "    - 테스트 세트의 목적은 최종 모델의 성능에 대한 편향되지 않은 추정을 제공하는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f31064",
   "metadata": {},
   "source": [
    "### VideoBias / Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66f713e",
   "metadata": {},
   "source": [
    "1. **바이어스(Bias)와 분산(Variance)의 이해**:\n",
    "   - 바이어스는 모델이 학습 데이터에 대해서 얼마나 잘 일반화되지 않는지를 나타냅니다. 높은 바이어스를 가진 모델은 데이터의 패턴을 충분히 학습하지 못해 과소적합(underfitting)이 발생합니다.\n",
    "   - 분산은 모델이 학습 데이터의 작은 변화에 얼마나 민감한지를 나타냅니다. 높은 분산을 가진 모델은 학습 데이터의 노이즈까지 너무 잘 학습해버려 과대적합(overfitting)이 발생합니다.\n",
    "\n",
    "2. **딥러닝에서의 바이어스/분산 트레이드오프**:\n",
    "   - 과거에는 바이어스와 분산 사이에는 트레이드오프 관계가 있다고 여겨졌습니다. 즉, 바이어스를 줄이면 분산이 증가하고, 분산을 줄이면 바이어스가 증가하는 경향이 있었습니다.\n",
    "   - 그러나 딥러닝에서는 이런 트레이드오프가 크게 관찰되지 않습니다. 복잡한 네트워크와 충분한 데이터를 이용하면 동시에 바이어스와 분산을 줄일 수 있습니다.\n",
    "\n",
    "3. **학습 오차와 개발 오차**:\n",
    "   - 학습 오차와 개발 오차를 비교함으로써 바이어스와 분산의 문제를 진단할 수 있습니다.\n",
    "   - 학습 오차가 높으면 바이어스가 높은 것이고, 학습 오차에 비해 개발 오차가 훨씬 높으면 분산이 높은 것입니다.\n",
    "\n",
    "4. **베이즈 오차**:\n",
    "   - 최적의 분류기로도 달성할 수 없는 최소의 오차를 말합니다. 이 값이 높은 경우, 바이어스와 분산을 진단하는 방법이 다를 수 있습니다.\n",
    "\n",
    "요약하자면, 바이어스와 분산은 머신러닝 모델의 성능을 평가하고 문제점을 진단하는 데 중요한 요소입니다. 이를 통해 모델이 학습 데이터에 과소적합하거나 과대적합하는지 알 수 있습니다. 따라서 바이어스와 분산의 개념을 잘 이해하는 것은 머신러닝의 성능을 향상시키는 데 도움이 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ae45da",
   "metadata": {},
   "source": [
    "### Basic Recipe for Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c09aa1f",
   "metadata": {},
   "source": [
    "머신 러닝 모델을 학습할 때, 우리는 주로 두 가지 큰 문제, 즉 **편향(bias)**과 **분산(variance)**을 직면합니다.\n",
    "\n",
    "1. **편향 문제(Bias Problem)**\n",
    "   - 편향이 높다는 것은 모델이 훈련 데이터를 잘 학습하지 못한다는 것을 의미합니다.\n",
    "   - 해결책:\n",
    "     - 더 큰 네트워크(더 많은 히든 레이어나 유닛) 사용\n",
    "     - 더 오래 학습\n",
    "     - 다른 최적화 알고리즘 사용\n",
    "     - 적합한 신경망 구조 찾기(이것은 실험적으로 접근해야 할 수 있음)\n",
    "\n",
    "2. **분산 문제(Variance Problem)**\n",
    "   - 분산이 높다는 것은 모델이 훈련 데이터에는 잘 맞지만 새로운 데이터(개발 세트)에는 잘 일반화하지 못한다는 것을 의미합니다.\n",
    "   - 해결책:\n",
    "     - 더 많은 훈련 데이터 수집\n",
    "     - 정규화(regularization) 사용\n",
    "     - 적합한 신경망 구조 찾기\n",
    "\n",
    "과거에는 편향과 분산 사이에 균형을 잡아야 하는 \"편향-분산 트레이드오프\"라는 개념이 있었습니다. 그러나 현대 딥 러닝에서는 큰 네트워크를 학습시키거나 더 많은 데이터를 얻는 방법으로 편향만 줄이거나 분산만 줄이는 방법을 갖게 되었습니다.\n",
    "\n",
    "따라서, 주어진 문제에 따라 편향 또는 분산 문제를 진단하고 적절한 접근 방식을 선택하는 것이 중요합니다. \n",
    "\n",
    "정규화는 네트워크가 과적합(overfitting)되는 것을 방지하는 데 유용한 기법입니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01c7bf6",
   "metadata": {},
   "source": [
    "# Regularizing your Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf38a54",
   "metadata": {},
   "source": [
    "### Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63fcc7b",
   "metadata": {},
   "source": [
    "신경망이 데이터에 과적합(overfitting)되는 것을 방지하기 위한 일반적인 방법 중 하나는 정규화(regularization)입니다. 정규화는 모델의 복잡성을 제한하여 훈련 데이터에 너무 딱 맞게 학습되는 것을 막습니다.\n",
    "\n",
    "**로지스틱 회귀에서의 정규화:**\n",
    "로지스틱 회귀에서의 비용 함수 $( J $)를 최소화하려고 합니다. 정규화를 추가하려면 비용 함수에 $( \\frac{\\lambda}{2m} \\times ||w||^2 $) 항을 추가합니다. 여기서 $( $lambda $)는 정규화 파라미터로, 너무 큰 값을 갖는 가중치 $(w)를 제한하기 위해 사용됩니다. \n",
    "\n",
    "$( ||w||^2 $)는 L2 노름이라고도 불리며, 이는 가중치의 각 요소를 제곱한 값의 합입니다. 이 정규화 방법은 L2 정규화라고도 합니다. L1 정규화의 경우 가중치의 절대값을 사용합니다.\n",
    "\n",
    "**신경망에서의 정규화:**\n",
    "신경망에서는 여러 층의 가중치 $( w[1], w[2], ... $)가 있습니다. 정규화를 추가하려면 모든 가중치 행렬에 대한 제곱합을 비용 함수에 추가합니다. 이 값은 프로베니우스 노름(Frobenius norm)으로 알려져 있으며, 행렬의 모든 요소의 제곱의 합입니다.\n",
    "\n",
    "정규화를 적용한 후 경사 하강법을 사용하여 가중치를 업데이트할 때 $( w $)에 대한 업데이트는 $( dw +frac{\\lambda}{m} \\times w $)에 비례하는 값으로 변경됩니다.\n",
    "\n",
    "이러한 정규화 방법을 사용하면 가중치가 감소하는 경향이 있습니다. 따라서 L2 정규화는 종종 'weight decay'라고도 불립니다.\n",
    "\n",
    "**왜 정규화는 과적합을 방지하는가?**\n",
    "정규화는 모델의 가중치를 제한하여 모델이 훈련 데이터에 과도하게 적응하는 것을 방지합니다. 가중치가 작아지면 모델의 복잡성이 감소하게 되므로 일반화 능력이 향상됩니다.\n",
    "\n",
    "마지막으로, $( $lambda $)는 하이퍼파라미터로서 개발 세트나 교차 검증을 통해 조정해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972e18ce",
   "metadata": {},
   "source": [
    "### Why Regularization Reduces Overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafb5c13",
   "metadata": {},
   "source": [
    "정규화는 기본적으로 모델이 학습 데이터에 너무 맞춰져 있어서 새로운 데이터에 대한 성능이 떨어지는 과적합 문제를 해결하기 위한 방법 중 하나입니다.\n",
    "\n",
    "1. **과적합의 예시**\n",
    "   - 고분산과 고편향: 고분산은 모델이 학습 데이터에 지나치게 적응하여 복잡한 경계선을 그리는 것을 의미하며, 고편향은 모델이 너무 단순해서 데이터의 패턴을 잡아내지 못하는 것을 의미합니다.\n",
    "   \n",
    "2. **정규화의 동작 원리**\n",
    "   - 정규화는 비용 함수에 패널티 항을 추가하여 모델의 가중치가 너무 큰 값을 가지지 않도록 합니다.\n",
    "   - L2 정규화는 Frobenius norm(프로베니우스 노름)을 사용하여 가중치의 크기를 패널티로 부과합니다.\n",
    "   \n",
    "3. **정규화가 과적합을 방지하는 직관적 이해**\n",
    "   - 큰 정규화 값 (λ)을 사용하면 가중치가 작아집니다. 따라서 신경망의 복잡성이 줄어들어 데이터에 과도하게 적응하는 것을 방지할 수 있습니다.\n",
    "   - 실제로 모든 은닉 유닛이 0으로 설정되는 것은 아니지만, 각 은닉 유닛의 효과가 작아져 신경망이 단순화됩니다.\n",
    "   \n",
    "4. **tan h 활성화 함수와의 관계**\n",
    "   - tan h 함수는 z의 값이 작을 때 선형과 유사합니다. 따라서 z의 값이 작으면 신경망은 선형적인 동작을 합니다.\n",
    "   - 큰 정규화 값 때문에 가중치와 z가 작아질 때, 신경망은 복잡한 비선형 경계선 대신 단순한 선형 경계선을 갖게 됩니다.\n",
    "   \n",
    "5. **구현 팁**\n",
    "   - 정규화를 사용할 때 비용 함수는 기존의 정의에서 추가된 패널티 항을 포함하게 됩니다.\n",
    "   - 그래디언트 디센트를 디버깅할 때는 이 새로운 비용 함수 정의를 사용해야 합니다.\n",
    "\n",
    "정리하면, 정규화는 모델의 가중치가 너무 큰 값을 가지는 것을 방지하여 모델이 학습 데이터에 과도하게 적응하는 것을 방지하며, 이를 통해 새로운 데이터에 대한 모델의 성능을 향상시키는 효과가 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92d2058",
   "metadata": {},
   "source": [
    "### Dropout Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3d0053",
   "metadata": {},
   "source": [
    "1. **Dropout의 기본 아이디어**:\n",
    "   - Dropout은 학습 과정 중에 신경망의 뉴런(또는 노드)을 임의로 꺼서(즉, 0으로 설정하여) 실제로 학습에 사용되지 않게 하는 기법입니다. 이렇게 하면 신경망이 특정 뉴런에 너무 의존하지 않게 되어 과적합을 줄일 수 있습니다.\n",
    "\n",
    "2. **실제 구현**:\n",
    "   - 예를 들어, 학습 중에 각 뉴런을 유지할 확률을 0.8로 설정하면, 각 뉴런은 80%의 확률로 유지되고 20%의 확률로 꺼집니다. 이를 구현하기 위해 각 뉴런에 대해 임의의 숫자를 생성하고 그 숫자가 0.8보다 작은지 확인하여 Dropout을 결정합니다.\n",
    "   - 이렇게 하면, 각 학습 단계에서 신경망의 서로 다른 부분이 꺼지게 되므로, 신경망은 데이터의 다양한 특징을 학습하게 됩니다.\n",
    "\n",
    "3. **Inverted Dropout**:\n",
    "   - Dropout을 사용하면 실제로 일부 뉴런이 비활성화되기 때문에, 활성화된 뉴런의 출력 값의 평균이 변경될 수 있습니다. 이를 보정하기 위해 \"Inverted Dropout\" 기법을 사용하여 활성화된 뉴런의 출력 값을 적절한 확률로 나눠줍니다.\n",
    "   - 예를 들어, 유지 확률이 0.8인 경우 출력 값을 0.8로 나누어 기대 출력 값을 일정하게 유지합니다.\n",
    "\n",
    "4. **테스트 시에는**:\n",
    "   - 모델을 실제로 사용할 때 (즉, 테스트나 실제 운영 환경에서) Dropout을 사용하지 않습니다. 왜냐하면 예측 시에는 모든 뉴런을 사용하여 가장 효과적인 성능을 얻기 위함입니다.\n",
    "\n",
    "Dropout의 이점은 모델이 훈련 데이터에 너무 특화되지 않게 하여 일반화 성능을 향상시키는 것입니다. 이는 딥 러닝 모델이 복잡하고 많은 수의 파라미터를 가질 때 특히 중요합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce9d85d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85f648a4",
   "metadata": {},
   "source": [
    "### Understanding Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f40774",
   "metadata": {},
   "source": [
    "**드롭아웃(Dropout)이란?**\n",
    "드롭아웃은 신경망 훈련 과정에서 일부 뉴런(유닛)을 임의로 꺼주는 기법입니다. 이 과정에서 신경망의 각 뉴런은 다른 특정 뉴런의 존재에 너무 의존적이지 않게 되며, 이로 인해 신경망의 과대적합(overfitting)을 방지하는 효과가 있습니다.\n",
    "\n",
    "**왜 드롭아웃이 잘 작동하는가?**\n",
    "1. **일종의 앙상블 학습**: 드롭아웃은 매번 다른 서브넷(subnet)을 사용하여 훈련시키는 것으로 볼 수 있습니다. 이렇게 다양한 작은 신경망들을 훈련시키는 것은 앙상블 학습의 일종과 유사하며, 일반화 능력을 향상시킵니다.\n",
    "\n",
    "2. **뉴런 간의 의존성 감소**: 뉴런의 입력 중 일부가 임의로 사라질 수 있기 때문에, 각 뉴런은 다른 특정 뉴런의 활성화에 의존하는 것을 피하게 됩니다. 이로 인해 가중치가 균형있게 조정되며, 이는 과대적합을 방지합니다.\n",
    "\n",
    "드롭아웃은 L2 정규화와 유사한 효과를 가지지만, L2 정규화와는 달리 가중치에 적용되는 정규화 정도가 다를 수 있습니다.\n",
    "\n",
    "**드롭아웃의 구현과 관련된 팁**:\n",
    "1. **계층별 Keep-prob**: 'keep-prob'은 드롭아웃 과정에서 뉴런을 유지할 확률을 의미합니다. 드롭아웃을 적용하는 각 계층마다 다른 'keep-prob' 값을 설정할 수 있습니다.\n",
    "\n",
    "2. **컴퓨터 비전과 드롭아웃**: 컴퓨터 비전 분야에서는 입력 데이터의 크기가 크기 때문에 과대적합 문제가 자주 발생합니다. 따라서 이 분야에서는 드롭아웃이 널리 사용됩니다.\n",
    "\n",
    "3. **비용 함수의 문제**: 드롭아웃을 사용하면 비용 함수 J의 값이 명확히 정의되지 않습니다. 이로 인해 훈련 과정을 시각적으로 모니터링하기 어려울 수 있습니다.\n",
    "\n",
    "결론적으로, 드롭아웃은 신경망이 과대적합하는 것을 방지하는 강력한 정규화 기법 중 하나입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff60c69",
   "metadata": {},
   "source": [
    "# Setting Up your Optimization Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00810b72",
   "metadata": {},
   "source": [
    "### Normalizing Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0eed8c",
   "metadata": {},
   "source": [
    "1. **정규화의 과정**:\n",
    "    - **평균 빼기**: 먼저, 트레이닝 세트의 모든 입력 값에서 평균을 빼서 평균이 0이 되게 합니다.\n",
    "    - **분산 정규화**: 다음으로, 각 입력 특성의 분산을 1로 조정합니다. 이는 각 입력 값을 해당 특성의 표준 편차로 나눔으로써 이루어집니다.\n",
    "    \n",
    "2. **테스트 세트에 대한 주의사항**: 훈련 데이터를 정규화할 때 사용한 동일한 평균과 표준 편차를 사용하여 테스트 데이터도 정규화해야 합니다. \n",
    "\n",
    "3. **왜 정규화가 중요한가?**: \n",
    "    - **비효율적인 최적화**: 입력 특성들이 다른 스케일을 가질 때, 비용 함수는 길쭉한 형태를 가지게 될 수 있습니다. 이런 형태의 비용 함수는 경사 하강법으로 최적화하는데 비효율적입니다. \n",
    "    - **효율적인 최적화**: 반면, 입력 특성들이 유사한 스케일을 가지면 비용 함수는 더 원형에 가까운 형태를 가집니다. 이런 형태의 비용 함수는 경사 하강법으로 더 빠르게 최적화할 수 있습니다.\n",
    "    - **훈련 속도 개선**: 정규화는 신경망의 훈련 속도를 크게 개선할 수 있습니다. 입력 특성들이 크게 다른 스케일을 가질 때 이 효과는 특히 두드러집니다.\n",
    "\n",
    "4. **일반적인 권장사항**: 입력 특성들이 비슷한 스케일을 가지고 있다면 정규화는 그렇게 중요하지 않을 수 있습니다. 그러나 대부분의 경우 정규화는 학습 속도를 향상시키며, 나쁜 영향을 미치는 경우는 거의 없습니다. \n",
    "\n",
    "요약하면, 정규화는 신경망의 훈련을 가속화하고, 최적화 과정을 개선하기 위해 사용되는 기술입니다. 입력 특성들이 다양한 범위나 스케일을 가질 때 특히 중요합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789f8d74",
   "metadata": {},
   "source": [
    "### Vanishing / Exploding Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3f4863",
   "metadata": {},
   "source": [
    "딥 뉴럴 네트워크, 특히 매우 깊은 네트워크를 훈련할 때 발생하는 문제 중 하나는 기울기가 폭발하거나 소실되는 것입니다. 이것은 깊은 네트워크를 훈련할 때 도함수나 기울기가 매우 크거나 매우 작아질 수 있음을 의미하며, 때로는 지수적으로 작아질 수 있습니다. 이로 인해 훈련이 어려워집니다.\n",
    "\n",
    "간단히 예를 들어, 우리가 선형 활성화 함수를 사용하고 편향(bias)을 무시한다고 가정해봅시다. 그럼 출력 Y는 W 행렬들의 연속된 곱과 입력 X의 곱으로 표현될 수 있습니다. \n",
    "\n",
    "만약 각 가중치 행렬이 단위 행렬보다 약간 큰 값, 예를 들어 1.5의 단위 행렬로 설정된다면, Y의 값은 지수적으로 크게 증가할 것입니다. 반대로, 각 행렬이 1보다 작은 값, 예를 들어 0.5의 단위 행렬로 설정되면, Y의 값은 지수적으로 감소합니다.\n",
    "\n",
    "이러한 통찰을 통해, 가중치 W가 단위 행렬보다 약간 크면 활성화는 폭발할 것이고, 가중치가 단위 행렬보다 약간 작으면 활성화는 지수적으로 감소한다는 것을 알 수 있습니다.\n",
    "\n",
    "딥 뉴럴 네트워크에서는 이러한 활성화 값이나 기울기 값이 지수적으로 증가하거나 감소할 수 있으므로, 이 문제를 해결하기 위한 방법이 필요합니다. 이 문제의 부분적인 해결책은 가중치 초기화를 신중하게 선택하는 것입니다. 신중한 가중치 초기화는 기울기의 폭발이나 소실 문제를 완전히 해결하지는 않지만 크게 도움을 줄 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb94fb95",
   "metadata": {},
   "source": [
    "### Weight Initialization for Deep Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b394279b",
   "metadata": {},
   "source": [
    "물론이죠, 앞서 언급된 내용을 한글로 요약하겠습니다.\n",
    "\n",
    "깊은 신경망에서는 기울기가 사라지거나 폭발하는 문제가 발생할 수 있습니다. 이 문제의 부분적인 해결 방법은 신경망의 가중치를 무작위로 초기화하는 방법을 더 신중하게 선택하는 것입니다.\n",
    "\n",
    "단일 뉴런의 예로 시작하여 깊은 네트워크로 일반화하는 방법을 살펴보겠습니다. 단일 뉴런에서는 x1부터 x4까지의 네 가지 특성을 입력으로 받을 수 있습니다. 그리고 이는 z를 통해 활성화 함수 g를 거쳐 y 출력으로 연결됩니다.\n",
    "\n",
    "z의 값이 너무 커지거나 작아지지 않게 하려면, Wi 값이 작아야 합니다. 왜냐하면 z는 Wi와 Xi의 합이기 때문입니다. 이를 바탕으로 W의 분산을 1/n으로 설정하는 것이 합리적입니다. 여기서 n은 뉴런에 입력되는 특성의 수입니다.\n",
    "\n",
    "ReLU 활성화 함수를 사용할 경우, 2/n의 분산을 사용하는 것이 더 좋은 결과를 가져올 수 있습니다. 다른 활성화 함수, 예를 들어 TanH를 사용할 경우 Xavier 초기화라고 불리는 다른 초기화 방법을 사용할 수 있습니다.\n",
    "\n",
    "이러한 초기화 방법들은 가중치가 너무 빨리 폭발하거나 0에 가까워지지 않게 하여 깊은 네트워크를 합리적으로 훈련할 수 있게 합니다. 이는 깊은 네트워크를 훈련할 때 더 빠르게 학습시키는 데 도움을 줍니다.\n",
    "\n",
    "이렇게 가중치 초기화는 심층 신경망의 훈련 속도를 향상시키는 또 다른 방법 중 하나입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1708ce",
   "metadata": {},
   "source": [
    "### Numerical Approximation of Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8021932",
   "metadata": {},
   "source": [
    "역전파(backpropagation)를 구현할 때 종종 그 구현이 정확한지 확인하기 위한 기술로 \"그래디언트 체크(gradient checking)\"가 사용됩니다. 이 기술은 네트워크에서 그래디언트를 수치적으로 근사하는 방법을 사용하여 역전파의 구현의 정확성을 검증합니다.\n",
    "\n",
    "1. **그래디언트의 수치 근사**: \n",
    "   - 기본 아이디어는 어떤 함수 f(θ)의 그래디언트를 근사하기 위해 θ를 약간 늘리거나 줄여보는 것입니다.\n",
    "   - θ에 대한 f의 기울기는 아래의 수식으로 계산됩니다:<br>\n",
    "     $[ \\frac{f(\\theta + \\epsilon) - f(\\theta - \\epsilon)}{2\\epsilon} $]\n",
    "   - 이 방법은 \"양방향 차이(two-sided difference)\"라고도 합니다.\n",
    "\n",
    "2. **높은 정확도**: \n",
    "   - 이 양방향 차이 방법은 단방향 차이 방법보다 그래디언트의 근사치가 훨씬 정확하다는 것이 확인되었습니다.\n",
    "   - 이는 칼큘러스의 이론에 기반하며, ε의 값이 작을수록 근사 오차가 ε^2에 비례하여 작아지기 때문입니다.\n",
    "\n",
    "3. **그래디언트 체크의 응용**: \n",
    "   - 이 방법은 신경망의 역전파 알고리즘이 올바르게 구현되었는지 확인하는 데 사용될 수 있습니다.\n",
    "   - 구체적으로는, 역전파로 계산된 그래디언트와 위의 수치 근사 방법으로 계산된 그래디언트를 비교합니다. 두 값이 매우 가까우면 역전파 구현이 올바른 것으로 판단됩니다.\n",
    "\n",
    "이해하기 쉽게 설명하자면, 그래디언트 체크는 역전파 알고리즘이 잘 동작하는지 확인하는 '디버깅 도구'와 같습니다. 역전파 구현에 미묘한 오류가 있을 때 그 오류를 찾아내기 위해 사용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3422bf",
   "metadata": {},
   "source": [
    "### Gradient Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e74c4c",
   "metadata": {},
   "source": [
    "Gradient checking은 기울기 검사라고 할 수 있는데, 이는 back propagation 구현에 있어서 버그를 찾는 데 많은 도움을 줍니다. \n",
    "\n",
    "신경망은 일반적으로 W1, B1 등의 많은 파라미터들로 구성되어 있습니다. 기울기 검사를 구현하려면 모든 파라미터를 하나의 큰 벡터 theta로 재구성해야 합니다. 이렇게 하면 비용 함수 J는 이제 큰 벡터 theta의 함수가 됩니다. \n",
    "\n",
    "그 다음, W와 B를 동일한 순서로 배열하여 dW[1], db[1] 등의 파라미터들을 취하고, 이들을 theta와 동일한 차원을 갖는 큰 벡터 d theta로 재구성합니다. 이제 우리의 목표는 d theta가 비용 함수 J의 기울기인지 확인하는 것입니다.\n",
    "\n",
    "grad check를 구현하기 위해서는 각 파라미터 theta_i에 대해 2-면 차이 (two-sided difference)를 사용하여 미분 값을 근사화합니다. 이렇게 하면 d theta approx라는 새로운 벡터를 얻게 됩니다. 우리는 이 d theta approx가 실제 d theta와 근사적으로 일치하는지 확인하려고 합니다.\n",
    "\n",
    "두 벡터가 얼마나 가까운지 확인하기 위해서는 두 벡터 사이의 유클리드 거리 (차이의 L2 norm)를 계산하고, 두 벡터의 길이로 정규화합니다. 일반적으로, 이 값이 10^-7보다 작으면 근사치가 매우 정확하다고 볼 수 있습니다. 10^-5 정도면 확인해봐야 할 수도 있고, 10^-3보다 크면 버그가 있을 가능성이 높습니다.\n",
    "\n",
    "기울기 검사는 버그를 찾고 디버깅하는 데 유용한 도구입니다. 따라서 신경망을 구현할 때 이 기술을 사용하면 back propagation 구현의 정확성을 높일 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7d904c",
   "metadata": {},
   "source": [
    "### Gradient Checking Implementation Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac3e9a4",
   "metadata": {},
   "source": [
    "\n",
    "1. **양쪽 차이 사용**:\n",
    "   - 단순한 전진 차이 (\\( \\frac{J(\\theta + \\epsilon) - J(\\theta)}{\\epsilon} \\)) 대신 중심 또는 양쪽 차이: \n",
    "     \\( \\frac{J(\\theta + \\epsilon) - J(\\theta - \\epsilon)}{2\\epsilon} \\)를 사용하십시오.\n",
    "   이는 그라디언트의 더 정확한 근사치를 제공합니다.\n",
    "\n",
    "2. **ε 값을 신중하게 선택**:\n",
    "   - ε (엡실론)에 대한 일반적인 선택은 \\(10^{-7}\\)입니다. 너무 작은 값은 정밀도 때문에 수치적 문제를 도입할 수 있고, 너무 큰 값은 좋은 근사치를 제공하지 않을 수 있습니다.\n",
    "\n",
    "3. **훈련 중에 사용하지 마십시오**:\n",
    "   - 그라디언트 체킹은 계산 비용이 많이 듭니다. 디버깅을 위해만 사용해야 합니다. 역전파 구현에 확신이 생기면 훈련 중에는 그라디언트 체킹을 끄십시오.\n",
    "\n",
    "4. **가끔 모든 매개변수 확인**:\n",
    "   - 매번 모든 단일 매개변수의 그라디언트를 확인할 필요는 없습니다. 그라디언트 체킹을 위해 매개변수의 하위 집합을 무작위로 선택할 수 있습니다. 그러나 시간이 지나면 모든 매개변수가 검사되도록 해야 합니다.\n",
    "\n",
    "5. **드롭아웃에 주의**:\n",
    "   - 신경망에서 드롭아웃을 사용하는 경우 그라디언트 체킹 중에는 드롭아웃을 끄십시오. 드롭아웃은 전방 및 역방향 전파에서 무작위성을 도입하여 원래의 역전파와 그라디언트 근사치 사이에 일관된 결과를 얻기 어렵게 만듭니다.\n",
    "\n",
    "6. **정규화**:\n",
    "   - 비용 함수에 정규화 항이 포함된 경우 \\( J(\\theta + \\epsilon) \\) 및 \\( J(\\theta - \\epsilon) \\)을 계산할 때 이를 포함시켜야 합니다.\n",
    "\n",
    "7. **상대 오차**:\n",
    "   - 역전파에서의 그라디언트와 수치 그라디언트를 비교할 때 상대 오차를 고려하십시오. 상대 오차가 작으면 (예: \\(10^{-7}\\) 이하) 역전파가 아마도 올바르다는 것을 의미합니다. \\(10^{-5}\\) 범위라면 주의해야 합니다. \\(10^{-3}\\)보다 크면 역전파에 오류가 있을 가능성이 높습니다.\n",
    "\n",
    "8. **오류 식별 및 분리**:\n",
    "   - 불일치가 발견되면 각 계층과 단위를 체계적으로 검사하십시오. 오류는 특정 계층이나 단위에 국한될 수 있습니다. 이는 문제가 있을 수 있는 위치를 좁히는 데 도움이 됩니다.\n",
    "\n",
    "9. **그라디언트 체킹 대 단위 테스트**:\n",
    "   - 그라디언트 체킹은 훌륭하지만, 다른 형태의 테스트가 필요하다는 것을 대체하지는 않습니다. 코드의 더 작고 독립적인 부분을 검사하기 위해 단위 테스트를 사용하는 것을 고려하십시오.\n",
    "\n",
    "10. **부동 소수점 정밀도**:\n",
    "   - 컴퓨터에서 부동 소수점 표현의 본질적인 제한 사항 때문에 계산된 값과 예상 값 사이의 미세한 차이점에 대해 인식하십시오.\n",
    "\n",
    "이 가이드라인과 방법론적인 접근법을 준수함으로써 그라디언트 체킹은 역전파 구현의 정확성을 확신하는 데 강력한 도구가 될 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9b3745",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

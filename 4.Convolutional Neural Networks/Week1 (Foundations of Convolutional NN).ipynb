{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "286b1c9f",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527a5783",
   "metadata": {},
   "source": [
    "### Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b80724",
   "metadata": {},
   "source": [
    "딥 러닝 컴퓨터 비전은 현재 자율 주행 자동차가 다른 자동차와 보행자를 감지하여 충돌을 피하는 데 도움을 주고, 얼굴 인식을 훨씬 효과적으로 작동하게 하고, 얼굴을 사용하여 스마트폰이나 문을 잠금 해제하는 등의 기능을 가능하게 합니다. 또한 스마트폰에서 음식 사진, 호텔 사진 또는 풍경 사진을 보여주는 앱들 중 많은 회사가 딥 러닝을 사용하여 사용자에게 가장 매력적이고 아름다운 사진을 보여주도록 도와줍니다. 또한 딥 러닝은 새로운 종류의 예술을 창조하는 데도 기여하고 있습니다.\n",
    "\n",
    "딥 러닝 컴퓨터 비전에 흥미를 느끼게 하는 두 가지 이유가 있습니다. 첫째, 컴퓨터 비전의 빠른 발전으로 몇 년 전에는 불가능했던 새로운 응용 프로그램을 개발할 수 있게 되었습니다. 따라서 이러한 도구를 배우면 여러분도 새로운 제품과 응용 프로그램 중 일부를 발명할 수 있을 것입니다. 둘째, 컴퓨터 비전 연구 커뮤니티가 새로운 신경망 아키텍처와 알고리즘을 개발하는 데 매우 창의적이고 발명가적이기 때문에 컴퓨터 비전 시스템을 직접 구축하지 않더라도 이 강의에서 배운 아이디어가 다른 영역으로의 크로스-페르틸리제이션을 촉진하는 것으로 나타납니다. 예를 들어 음성 인식을 개발할 때 컴퓨터 비전 아이디어에서 영감을 얻어 음성 문헌에 적용하기도 합니다.\n",
    "\n",
    "이 강의에서 다루는 몇 가지 컴퓨터 비전 문제 예제는 다음과 같습니다.\n",
    "\n",
    "1. 이미지 분류(Image Classification): 이미지를 입력으로 받아서 고양이인지 아닌지 등을 판별합니다.\n",
    "2. 객체 감지(Object Detection): 예를 들어 자율 주행 자동차를 구축하는 경우, 다른 자동차의 위치를 파악하여 충돌을 피하도록 해야 합니다.\n",
    "3. 신경 스타일 전송(Neural Style Transfer): 내용 이미지와 스타일 이미지를 입력으로 받아, 내용 이미지를 스타일 이미지의 스타일로 다시 그립니다.\n",
    "\n",
    "컴퓨터 비전 문제의 한 가지 어려움은 입력 데이터가 매우 크다는 것입니다. 이를 극복하기 위해 컨볼루션 연산(Convolution Operation)을 효과적으로 구현해야 합니다. 큰 이미지를 다루기 위해서는 컨볼루션 연산을 개선해야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06059218",
   "metadata": {},
   "source": [
    "### Edge Detection Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342b8fb3",
   "metadata": {},
   "source": [
    "컨볼루션(Convolution) 연산이 컨볼루션 네트워크(Convolutional Neural Network)의 기본 구성 요소 중 하나임을 설명하고, 가장자리(Edge) 검출을 예시로 사용하여 컨볼루션 연산이 어떻게 작동하는지 설명합니다.\n",
    "\n",
    "이전 동영상에서는 초기 신경망 레이어가 가장자리를 감지하고 그 후 몇몇 레이어가 객체의 유사한 부분을 감지하고 마지막 레이어에서는 완전한 객체를 감지할 수 있다는 것을 언급했습니다. 이 동영상에서는 이미지에서 어떻게 가장자리를 감지하는지 보여줍니다.\n",
    "\n",
    "예를 들어, 주어진 이미지에서 객체를 찾기 위해 컴퓨터에게 수행할 수 있는 첫 번째 단계 중 하나는 이미지 내의 수직 가장자리를 감지하는 것입니다. 예를 들어, 건물이 있는 곳에는 수직 선이 있고 보행자들은 수직한 가로 모양을 이루므로 수직 가장자리를 감지할 수 있습니다. 수평 가장자리도 감지할 수 있습니다. 예를 들어, 난간이 있는 곳에는 매우 강한 수평 선이 있으므로 수평 가장자리도 감지됩니다.\n",
    "\n",
    "가장자리를 어떻게 감지할까요? 예를 들어 6x6 흑백 이미지를 가지고 수직 가장자리를 감지하려면 3x3 행렬(필터 또는 커널이라고도 함)을 만들 수 있습니다. 이 행렬은 다음과 같습니다.\n",
    "```\n",
    "1  1  1\n",
    "0  0  0\n",
    "-1 -1 -1\n",
    "```\n",
    "이제 6x6 이미지를 3x3 필터와 컨볼루션 연산을 수행합니다. 컨볼루션 연산은 '*'로 표시되며 결과적으로 4x4 행렬을 얻습니다. 첫 번째 원소를 계산하는 방법은 3x3 필터를 원본 입력 이미지의 3x3 영역 위에 놓는 것입니다. 그리고 각 요소별 곱셈을 수행하고 그 결과를 모두 더합니다. 이러한 과정을 통해 4x4 출력 행렬을 얻습니다.\n",
    "\n",
    "이 컨볼루션 연산을 수행하면 결과적으로 수직 가장자리가 감지된 4x4 이미지가 생성됩니다. 수직 가장자리를 검출하는 이유는 입력 이미지의 왼쪽 부분이 밝고 오른쪽 부분이 어두운 부분에서 수직 가장자리가 감지되기 때문입니다.\n",
    "\n",
    "이런 식으로 컨볼루션 연산을 사용하면 이미지에서 수직 가장자리를 검출할 수 있습니다. 이것은 컴퓨터 비전에서 가장자리를 찾는 간편한 방법입니다. 다음 동영상에서는 이러한 컨볼루션 연산을 컨볼루션 네트워크의 기본 구성 요소 중 하나로 사용하는 방법을 자세히 살펴볼 것입니다.\n",
    "\n",
    "이 동영상에서는 컨볼루션 연산을 사용하여 수직 가장자리 감지자를 구현하는 방법을 보았습니다. 이제는 양의 가장자리와 음의 가장자리 간의 차이, 즉 밝음에서 어둠 또는 어둠에서 밝음으로의 가장자리 전이의 차이와 같이 양쪽 가장자리를 이해하고 다른 유형의 가장자리 감지자 및 우리가 지금까지 수동으로 가장자리 감지자를 코딩하는 대신 알고리즘을 학습시키는 방법을 배울 것입니다.\n",
    "\n",
    "먼저 이전 동영상에서 보았던 예제를 살펴보겠습니다. 6x6 이미지에서 왼쪽은 밝고 오른쪽은 어둡다는 이미지가 있었고, 수직 가장자리 감지 필터와 컨볼루션을 수행하면 이미지 중앙을 따라 수직 가장자리를 감지한 결과가 나옵니다.\n",
    "\n",
    "다음으로, 이미지 색상이 반대로 뒤집힌 이미지에서 어떤 일이 발생하는지 살펴봅니다. 이제 10은 이미지의 오른쪽 절반에 있고 0은 왼쪽에 있습니다. 동일한 가장자리 감지 필터와 컨볼루션을 수행하면 30 대신 -30이 나오며, 이것을 그림으로 표현하면 다음과 같을 것입니다. 전환의 그림자가 반전되었기 때문에 30도 반전되었습니다. 음의 30은 이것이 밝음에서 어둠으로의 전이가 아니라 어둠에서 밝음으로의 전이임을 나타냅니다. 두 경우 중 어느 쪽이든 상관없다면 이 출력 행렬의 절대값을 취할 수 있습니다. 그러나 이 특정 필터는 밝음에서 어둠으로의 가장자리와 어둠에서 밝음으로의 가장자리 간의 차이를 나타냅니다.\n",
    "\n",
    "그 다음으로, 가장자리 감지의 더 많은 예를 살펴보겠습니다. 우리가 본 3x3 필터는 수직 가장자리를 감지하는 데 사용할 수 있다는 것이 분명하므로 이 3x3 필터는 수평 가장자리를 감지하는 데도 사용될 수 있습니다. 수직 가장자리 감지 필터에 따르면 수직 가장자리는 왼쪽 부분이 상대적으로 밝고 오른쪽 부분이 어두운 3x3 영역입니다. 따라서 유사하게, 수평 가장자리는 상단에 상대적으로 밝은 픽셀과 하단 행에 상대적으로 어두운 픽셀을 가진 3x3 영역일 것입니다.\n",
    "\n",
    "이러한 원리를 바탕으로 수평 가장자리를 감지하는 예를 살펴보겠습니다. 왼쪽 상단과 오른쪽 하단 모서리에 10이 있는 더 복잡한 예시입니다. 이를 이미지로 그리면, 0이 있는 어두운 영역과 상단 왼쪽 및 하단 오른쪽 코너에 밝은 영역이 있을 것입니다. 이를 수평 가장자리 감지 필터와 컨볼루션하면 결과가 다음과 같습니다.\n",
    "\n",
    "다양한 가장자리 감지자를 사용하여 다양한 종류의 가장자리를 감지할 수 있으며, 컴퓨터 비전 문헌에서는 가장 좋은 숫자 세트가 무엇인지에 대한 논쟁이 있었습니다. 히스토리컬하게는 여러 종류의 숫자 세트를 사용해볼 수 있습니다. 예를 들어, 1, 2, 1, 0, 0, 0, -1, -2, -1을 사용할 수 있으며 이를 소벨 필터(Sobel filter)라고 합니다. 이 필터는 중심 픽셀에 약간 더 무게를 둔다는 이점이 있어 좀 더 견고하게 감지할 수 있습니다. 컴퓨터 비전 연구자는 이외에도 다른 숫자 세트를 사용하기도 합니다. 예를 들어, 1, 2, 1 대신 3, 10, 3, 그리고 -3, -10, -3을 사용하는 것이 샤르 필터(Scharr filter)라고 할 수 있습니다. 이는 수직 가장자리 감지에 대한 것이며, 이를 90도 회전하면 수평 가장자리 감지가 됩니다.\n",
    "\n",
    "그리고 딥 러닝의 등장으로, 복잡한 이미지에서 가장자리를 검출할 때 이러한 아홉 개의 숫자를 컴퓨터 비전 연구자가 직접 선택할 필요가 없을 수도 있습니다. 이 아홉 개의 숫자를 파라미터로 취급하고 역전파를 사용하여 학습할 수 있다는 아이디어가 나왔습니다. 목표는 이미지를 가져와서 3x3 필터로 컨볼루션하면 좋은 가장자리 감지자가 생성되도록 이 아홉 개의 파라미터를 학습하는 것입니다.\n",
    "\n",
    "나중 동영상에서 보게 되겠지만, 이 아홉 개의 숫자를 파라미터로 취급하고 이를 데이터로부터 자동으로 학습할 수 있는 역전파 방법에 대한 자세한 내용을 다룰 것입니다. 하지만 먼저 기본 컨볼루션 연산의 패딩(padding) 및 다양한 스트라이드(stride)에 대한 다른 변형에 대해 이야기하고 이것이 컨볼루션 네트워크의 기본 구성 요소 중요한 부분이라는 것을 이해하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a652f70b",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4917d36",
   "metadata": {},
   "source": [
    "딥 뉴럴 네트워크에서 기본 합성곱 연산에 대한 중요한 수정 사항 중 하나는 패딩(padding)의 사용입니다. 패딩이 어떻게 작동하는지 이해해 봅시다. 이전 비디오에서는 6x6 이미지를 3x3 필터로 합성할 때 4x4 출력이 생성되는 것을 보았습니다. 이는 3x3 필터가 6x6 이미지 내에서 맞을 수 있는 가능한 위치가 4x4뿐이기 때문입니다. 이에 대한 수학적인 관계는 n x n 이미지를 f x f 필터와 합성할 때 출력의 차원은 (n - f + 1) x (n - f + 1)이라는 것입니다.\n",
    "\n",
    "여기에는 두 가지 단점이 있습니다. 첫째, 합성 곱 작업을 적용할 때마다 이미지 크기가 축소되므로 깊은 신경망에서 문제가 될 수 있으며, 많은 레이어를 거친 후에 이미지 크기가 매우 작아집니다. 둘째, 이미지의 모서리나 가장자리의 픽셀은 오직 필터의 하나의 위치에서만 사용되므로 출력에 덜 기여합니다.\n",
    "\n",
    "이러한 문제를 해결하기 위해 이미지에 패딩(padding)을 적용할 수 있습니다. 패딩을 적용하면 본질적으로 이미지 주위에 추가로 행과 열을 0으로 채우게 됩니다. 예를 들어, 이미지 주위에 한 픽셀의 추가 테두리를 적용하면 6x6 이미지에 대한 8x8 이미지가 생성됩니다. 8x8 이미지를 3x3 필터와 합성할 때 여전히 6x6 출력을 얻게 되어 원래 입력 크기를 보존합니다.\n",
    "\n",
    "관례적으로 이미지를 패딩할 때는 이미지를 0으로 채우는 것이 일반적입니다. \"p\"가 패딩 양인 경우, 출력 차원은 (n + 2p - f + 1) x (n + 2p - f + 1)이됩니다. 주어진 예에서 p는 이미지 주위에 추가 픽셀 하나의 테두리를 패딩했으므로 p는 1입니다. 따라서 출력은 (6 + 2*1 - 3 + 1) x (6 + 2*1 - 3 + 1) = 6x6으로 원래 크기를 보존합니다.\n",
    "\n",
    "패딩의 효과는 이미지 가장자리에 있는 픽셀이 출력의 여러 셀에 더 영향을 주도록 하여 가장자리 픽셀의 정보 손실 문제를 감소시킵니다.\n",
    "\n",
    "패딩에 대한 두 가지 일반적인 선택 사항은 \"Valid 컨볼루션(패딩 없음)\"과 \"Same 컨볼루션(입력과 출력 크기가 동일하게 유지되도록 패딩)\"입니다. Valid 컨볼루션에서는 패딩이 없으므로 출력 크기가 더 작아집니다 (p = 0). Same 컨볼루션에서는 입력 크기와 출력 크기가 동일하게 유지되도록 패딩이 적용됩니다. 이를 위해 사용하는 공식은 f가 홀수일 때 p = (f - 1) / 2 입니다. 이 공식을 사용하면 출력 크기가 입력 크기와 동일하게 유지됩니다.\n",
    "\n",
    "컴퓨터 비전에서 필터 크기 (f)는 주로 홀수로 선택되며 패딩 프로세스를 단순화하고 홀수 크기 필터에서 중앙 픽셀을 정의하는 것이 더 쉽기 때문에 이러한 관례적인 선택 사항을 사용하는 것이 일반적입니다. 3x3, 5x5 등과 같이 홀수 필터 크기를 사용하는 것이 일반적입니다.\n",
    "\n",
    "따라서 컨볼루션 작업의 패딩을 지정하려면 패딩 값을 \"p\"로 설정하거나 \"Valid\" 또는 \"Same\" 컨볼루션 중 어느 것인지 지정하면 됩니다. 패딩은 이미지 크기 감소 및 가장자리 픽셀의 불균형한 기여에 관련된 문제를 해결하는 데 도움이 됩니다. 다음 비디오에서는 스트라이드 컨볼루션을 어떻게 구현하는지에 대해 논의하겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72826524",
   "metadata": {},
   "source": [
    "### Strided Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8090e55c",
   "metadata": {},
   "source": [
    "스트라이드 컨볼루션(Strided Convolution)은 컨볼루션 신경망(Convolutional Neural Networks)의 기본 구성 요소 중 하나입니다. 간단한 예제를 통해 이해해보겠습니다. 7x7 이미지를 3x3 필터로 컨볼루션하려고 합니다. 다만, 보통과 다르게 스트라이드(stride)를 2로 설정하겠습니다. 이것은 일반적인 방법과 마찬가지로 왼쪽 상단 3x3 영역에서 원소별 곱셈을 수행한 다음 덧셈하여 91을 얻습니다. 그러나 파란색 상자를 한 번 이동시키는 대신에 우리는 이를 두 번 이동시킬 것입니다. 파란색 상자를 2개 픽셀씩 이동시킵니다. 파란색 상자의 시작 위치가 왼쪽 상단 모서리에서 오른쪽 모서리로 이동했음을 주목하세요. 그런 다음 원래와 같이 원소별 곱셈과 합산을 수행하면 100이 됩니다. 다시 한 번 같은 작업을 수행하고 파란색 상자를 2개 픽셀씩 이동시키면 다음 위치에서 83을 얻습니다. 이제 다음 행으로 이동하면 다시 한 번 파란색 상자를 2개 픽셀씩 이동시킵니다. 한 위치를 건너뛰고 이동합니다. 그리고 이로부터 69를 얻습니다. 이제 또 다시 2개 픽셀씩 이동시켜 91을 얻습니다. 이렇게 진행하면 127을 얻습니다. 마지막 행에 대해서도 동일한 작업을 수행하며 44, 72, 74를 얻게 됩니다. 이 예에서는 7x7 행렬을 3x3 필터로 합성한 결과로 3x3 출력이 생성되었습니다. 입력 및 출력 차원은 다음과 같은 공식으로 결정됩니다. n x n 크기의 이미지를 f x f 크기의 필터로 합성하는 경우, 패딩 p와 스트라이드 s를 사용하면 출력이 다음과 같은 차원을 갖게 됩니다. 이 예에서 s는 2로 설정되었습니다.\n",
    "\n",
    "출력 크기 = (n + 2p - f) / s + 1\n",
    "\n",
    "예를 들어, 이 예에서 n = 7, f = 3, p = 0, s = 2라고 가정하면 출력 크기는 다음과 같이 계산됩니다.\n",
    "\n",
    "출력 크기 = (7 + 2*0 - 3) / 2 + 1 = (7 - 3) / 2 + 1 = 4 / 2 + 1 = 2 + 1 = 3\n",
    "\n",
    "따라서 3x3 출력이 생성됩니다. 이 공식에서 주의할 점은 소수점 부분은 버린다는 것입니다. 이는 바닥 함수(floor function)로 나타낼 수 있으며, 소수 z를 가장 가까운 정수 값으로 내림합니다.\n",
    "\n",
    "요약하면, n x n 행렬 또는 이미지를 f x f 행렬 또는 필터로 합성하는 경우, 패딩 p와 스트라이드 s를 사용하여 출력 크기를 계산할 수 있습니다. 이러한 값을 선택하여 정수값이 되도록 할 수 있지만, 경우에 따라 반올림하여 정수값이 아닌 경우도 있습니다. 하지만 일반적으로는 반올림된 정수값을 사용하며, 대부분의 경우 이러한 공식이 출력 크기를 올바르게 계산하는 데 충분합니다.\n",
    "\n",
    "또한, 크로스-코릴레이션(cross-correlation) 대신 컨볼루션(convolution)을 사용하는 관례적인 표기법에 대한 기술적인 주석을 추가하고 싶습니다. 이것은 컨볼루션 신경망을 구현하는 데 영향을 미치지 않지만, 수학 교재나 신호 처리 교재에 따라 표기법에 다소 불일치성이 있을 수 있음을 나타냅니다. 일반적인 수학 교재에서 컨볼루션을 정의할 때 원소별 곱셈과 합산을 수행하기 전에 실제로 수행하는 단계가 하나 더 있습니다. 이 단계는 6x6 행렬과 3x3 필터를 컨볼루션할 때 3x3 필터를 먼저 수평 및 수직 축을 기준으로 뒤집는 것입니다. 이것은 필터 행렬을 수직 및 수평 축 양쪽으로 뒤집는 것입니다. 그런 다음 이를 뒤집힌 행렬로 복사하여 상단 좌측 대부분의 4x4 출력 원소를 계산하는 데 사용됩니다. 그런 다음 이러한 아홉 개의 숫자를 가져와 한 칸 이동시킵니다. 이렇게 정의한 컨볼루션 연산은 원래의 컨볼루션 대신 크로스-코릴레이션 연산을 수행하는 것이 사실입니다. 그러나 딥 러닝 문헌에서는 일반적으로 이를\n",
    "\n",
    " 컨볼루션 연산이라고 부릅니다. 딥 러닝 분야에서는 이러한 뒤집기 작업을 생략하고 대부분 컨볼루션 연산이라고 부릅니다. 이것은 신호 처리 응용 프로그램에는 유용한 속성이지만, 딥 뉴럴 네트워크에서는 큰 차이를 만들지 않으며 코드를 단순화하고 더 잘 작동합니다. 따라서 대부분은 이것을 컨볼루션이라고 부릅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee650c4",
   "metadata": {},
   "source": [
    "### Convolutions Over Volume"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcecf0a",
   "metadata": {},
   "source": [
    "이제 3차원 볼륨(3D Volume) 상에서 컨볼루션을 구현하는 방법을 살펴보겠습니다. 이전에 2D 이미지에 대한 컨볼루션을 살펴봤으며, 이제 3D 볼륨 상에서 컨볼루션을 어떻게 수행하는지 알아보겠습니다.\n",
    "\n",
    "예를 들어, 단순한 회색조 이미지가 아닌 RGB 이미지에서 특징을 감지하고 싶다고 가정해 봅시다. RGB 이미지는 6x6 이미지 대신 6x6x3 형식일 수 있으며, 여기서 3은 세 가지 색상 채널에 해당합니다. 이를 세 개의 6x6 이미지 스택으로 생각할 수 있습니다. 이미지의 높이는 6, 너비는 6이며 채널 수는 3입니다. 이제 3D 이미지에서 엣지(edge) 또는 다른 특징을 감지하려면 이전과 같이 3x3 필터가 아닌 3D 필터를 사용합니다. 이 필터는 3x3x3이 될 것이며, 필터 자체에는 빨간색, 녹색 및 파란색 채널에 해당하는 세 개의 레이어가 있을 것입니다. 필터를 이루는 3D 볼륨은 높이, 너비 및 채널 수를 가집니다. 이미지의 채널 수와 필터의 채널 수가 일치해야하므로 이 두 숫자는 동일해야 합니다. 컨볼루션 연산의 결과는 4x4 이미지가 될 것이며, 여기서 4x4x1이며, 3이 더 이상 존재하지 않습니다.\n",
    "\n",
    "컨볼루션 연산을 실제로 어떻게 수행하는지 자세히 살펴보겠습니다. 좀 더 깔끔한 그림을 사용하여 설명하겠습니다. 6x6x3 이미지와 3x3x3 필터를 사용한다고 가정합니다. 필터를 표현하기 위해 이 3x3x3 필터를 스택으로 그리는 대신, 때로는 이를 3차원 큐브로 그릴 때도 있습니다. 이 컨볼루션 작업의 출력을 계산하기 위해서는 3x3x3 필터를 먼저 왼쪽 상단 위치에 배치합니다. 이 3x3x3 필터에는 27개의 숫자 또는 매개변수가 있으며, 이는 3개의 큐브입니다. 따라서 이 27개의 숫자 중 각각을 이미지의 빨간 채널, 녹색 채널 및 파란 채널에서 가져와서 왼쪽에 표시된 노란색 큐브에 의해 덮어쓰는 27개의 숫자와 곱합니다. 그런 다음 이 숫자를 모두 더하여 출력의 첫 번째 숫자를 얻습니다. 그 다음 출력을 계산하기 위해 이 큐브를 한 칸 이동시키고 다시 27개의 곱셈을 수행하고 숫자를 모두 더하여 다음 출력을 얻습니다. 다음 위치에 대한 다음 출력을 계산합니다. 그림을 그릴 때부터 더해가는 것이 아니라 하나의 큐브를 이동하면서 출력을 계산합니다. 그리고 한 줄 아래로 이동하여 계속 진행합니다. 그것이 다음 출력을 얻고 다음 출력을 얻고 다음 출력을 얻고 이런 식으로 진행합니다. 이 작업을 계속 하면 가장 오른쪽 아래 위치에 마지막 출력이 나올 것입니다.\n",
    "\n",
    "이러한 3x3x3 필터를 사용하면 어떤 일을 할 수 있을까요? 예를 들어, 이 필터는 3x3x3 필터에서 빨간색 채널에서 엣지를 감지하려면 다음과 같이 첫 번째 필터를 설정할 수 있습니다. 첫 번째 필터가 모든 채널에서 1, 1, 1로 설정되고, 녹색 채널 및 파란색 채널이 모두 0으로 설정됩니다. 이러한 세 개의 필터를 함께 사용하여 3x3x3 필터를 만들면 빨간색 채널에서만 수직 엣지를 감\n",
    "\n",
    "지하는 필터가 됩니다. 또한 어떤 색상에서든 수직 엣지를 감지하려면 다음과 같이 다른 설정을 할 수 있습니다. 이것은 세 개의 채널에서 1, 1, 1, -1, -1, -1로 설정되어 있으며 모든 채널에서 모든 엣지를 감지할 수 있도록 합니다. 이런 방식으로 이러한 매개변수를 설정함으로써 3x3x3 필터에서 다양한 특징 감지기를 얻을 수 있습니다.\n",
    "\n",
    "컴퓨터 비전에서 컨볼루션의 입력 높이, 너비 및 채널 수에 따라 필터의 높이와 너비가 달라질 수 있다는 점에 유의하십시오. 이것은 출력이 n - f + 1 x n - f + 1 x nC'로 계산된다는 것을 의미합니다. 이 예에서 nC'는 필터의 수, 즉 감지기의 수를 나타냅니다. 기본적으로 stride가 1이고 패딩이 없다고 가정한 것이며, 다른 stride 또는 패딩을 사용하면 n - f + 1 값이 일반적으로 변경됩니다.\n",
    "\n",
    "이제 볼륨 상에서 컨볼루션을 수행하는 아이디어가 강력하다는 것을 이해하셨을 것입니다. 단순히 RGB 이미지에서 직접 작업할 수 있는 작은 부분 뿐만 아니라, 여러 특징을 감지할 수 있으므로 수직 엣지, 수평 엣지뿐만 아니라 45도 엣지, 70도 엣지와 같은 다양한 엣지를 감지하려면 어떨까요? 다른 여러 필터를 동시에 사용하려면 어떻게 해야 할까요? 이것은 다음에 다루게 될 것이며, 다중 필터를 사용하면 감지기를 여러 개 동시에 사용할 수 있으며 출력은 필터의 수에 따라 다른 채널을 가질 것입니다. 이러한 다중 필터의 사용은 딥 뉴럴 네트워크(DNN)를 구축하는 데 필수적인 중요한 아이디어 중 하나입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5ee8d8",
   "metadata": {},
   "source": [
    "### One Layer of a Convolutional Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f1a494",
   "metadata": {},
   "source": [
    "1. **필터로 합성곱**: CNN에서는 필터(커널)를 사용하여 3D 입력 볼륨에 합성곱 작업을 적용합니다. 이러한 필터는 입력 위를 슬라이딩하며 요소별 곱셈을 수행하여 특징을 감지합니다.\n",
    "\n",
    "2. **다중 필터**: 서로 다른 특징을 감지하기 위해 여러 필터를 동시에 사용할 수 있습니다. 각 필터는 별도의 출력을 생성하여 여러 채널을 갖는 출력 볼륨을 생성합니다.\n",
    "\n",
    "3. **편향과 비선형성 추가**: 합성곱 연산 이후에는 출력의 각 요소에 편향 항을 더하고 비선형 활성화 함수(ReLU 등)를 적용하여 비선형성을 도입합니다.\n",
    "\n",
    "4. **출력 크기**: CNN의 한 계층의 출력 크기는 패딩(padding) 및 스트라이드(stride)와 같은 요소에 따라 입력과 다를 수 있습니다. 출력 크기는 필터 크기(f), 패딩(p), 스트라이드(s)와 관련된 공식을 사용하여 결정됩니다.\n",
    "\n",
    "5. **파라미터**: 계층 내의 각 필터는 필터 크기에 입력 채널 수를 곱한 값과 편향 항을 가집니다. 계층 내의 총 파라미터 수는 사용된 필터 수에 따라 결정됩니다.\n",
    "\n",
    "6. **표기법**: CNN 계층의 주요 매개변수를 설명하기 위한 일관된 표기법이 소개되었습니다. 이 표기법은 필터 크기(f), 패딩(p), 스트라이드(s), 입력 크기(n_h, n_w, n_c), 및 출력 크기(n_hl, n_wl, n_cl)를 설명하는 데 도움이 됩니다. 이러한 계층의 차원과 속성을 설명하는 데 도움이 되는 표기법입니다.\n",
    "\n",
    "7. **파라미터 보존**: CNN의 장점 중 하나는 입력 이미지의 크기에 관계없이 파라미터 수가 일정하게 유지되므로 모델의 복잡성을 크게 증가시키지 않고 다양한 크기의 이미지를 처리하기에 적합하다는 것입니다.\n",
    "\n",
    "8. **변수 순서**: 높이, 너비 및 채널 차원의 순서는 다양한 프레임워크 및 구현에서 다를 수 있지만 특정 컨텍스트 내에서 일관성을 유지하는 것이 중요합니다.\n",
    "\n",
    "이러한 표기법이 복잡하게 보일 수 있지만, 처음부터 모두 외우는 것은 필수가 아닙니다. 연습을 통해 CNN에 더 익숙해지면 이러한 개념에 대한 실전 경험을 얻게 될 것입니다. 중요한 점은 CNN의 한 계층이 어떻게 입력을 처리하여 출력을 생성하는지 이해하고 관련된 매개변수를 계산하는 방법을 이해하는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b796c4",
   "metadata": {},
   "source": [
    "### Simple Convolutional Network Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f69f7b",
   "metadata": {},
   "source": [
    "1. **이미지 분류 문제**: 이미지 분류 또는 이미지 인식 문제를 다루기 위한 CNN 예제를 고려합니다. 입력 이미지 x를 가져와서 이것이 고양이인지 아닌지를 판단하는 이진 분류 작업입니다.\n",
    "\n",
    "2. **입력 이미지 크기**: 예제에서는 작은 이미지를 사용합니다. 39 x 39 x 3 크기의 이미지를 고려합니다. 입력 이미지는 39 픽셀의 높이와 너비를 갖으며, 3개의 채널(RGB 컬러 이미지)을 가집니다.\n",
    "\n",
    "3. **첫 번째 합성곱 계층**: 첫 번째 계층에서는 3x3 필터를 사용하고 스트라이드(stride)는 1, 패딩(padding)은 없으며, 10개의 필터를 사용합니다. 결과적으로 출력 크기는 37 x 37 x 10이 됩니다.\n",
    "\n",
    "4. **두 번째 합성곱 계층**: 다음 계층에서는 5x5 필터를 사용하고 스트라이드는 2, 패딩은 없으며, 20개의 필터를 사용합니다. 출력 크기는 17 x 17 x 20이 됩니다.\n",
    "\n",
    "5. **세 번째 합성곱 계층**: 마지막 계층에서는 다시 5x5 필터를 사용하고 스트라이드는 2, 패딩은 없으며, 40개의 필터를 사용합니다. 출력 크기는 7 x 7 x 40이 됩니다.\n",
    "\n",
    "6. **플래튼(Flatten) 계층**: 마지막 합성곱 계층의 출력을 1,960개의 유닛으로 펼칩니다. 이를 통해 로지스틱 회귀 유닛 또는 소프트맥스(Softmax) 유닛에 입력으로 제공하여 최종 예측을 수행합니다.\n",
    "\n",
    "7. **하이퍼파라미터 선택**: CNN을 디자인할 때 중요한 작업 중 하나는 하이퍼파라미터를 선택하는 것입니다. 이에는 필터 크기, 스트라이드, 패딩, 필터 수 등이 포함됩니다.\n",
    "\n",
    "8. **출력 크기 및 채널**: 각 계층에서의 출력 크기와 채널 수는 계산되고, 이 정보는 CNN의 구조를 정의하는 데 중요합니다.\n",
    "\n",
    "9. **Pooling 및 Fully Connected 계층**: CNN은 일반적으로 합성곱 계층 외에도 풀링(Pooling) 계층과 완전 연결(Fully Connected) 계층을 포함합니다. 이 레이어들은 나중에 다룰 예정입니다.\n",
    "\n",
    "10. **추가 지침**: 하이퍼파라미터 선택에 대한 추가 지침은 이후 비디오에서 다룰 예정입니다.\n",
    "\n",
    "CNN은 이미지 처리 작업에서 효과적으로 사용되는 강력한 신경망 아키텍처 중 하나이며, 이미지 특징을 자동으로 감지하고 분류하는 데 사용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3355dd",
   "metadata": {},
   "source": [
    "### Pooling Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cbf000",
   "metadata": {},
   "source": [
    "풀링 레이어는 표현의 크기를 줄이고 연산 속도를 빠르게 하며, 일부 특징을 더 견고하게 만드는 데 사용됩니다.\n",
    "\n",
    "1. **Max 풀링(Max Pooling)의 예시**: Max 풀링은 주로 사용되는 풀링 방법 중 하나입니다. 4x4 입력을 받아 2x2 출력을 생성하는 예시를 살펴봅니다. 입력을 2x2 영역으로 나누고 각 영역에서 최대값을 선택하여 출력을 생성합니다. 이러한 풀링 작업은 주로 특징의 감지를 유지하면서 공간 크기를 줄이는 역할을 합니다.\n",
    "\n",
    "2. **Max 풀링의 하이퍼파라미터**: Max 풀링의 하이퍼파라미터로는 필터 크기(f)와 스트라이드(s)가 있습니다. 필터 크기가 2x2이고 스트라이드가 2x2이면, 출력 크기가 줄어들게 됩니다.\n",
    "\n",
    "3. **Max 풀링의 역할**: Max 풀링은 주어진 영역에서 가장 큰 특징을 선택하여 그 특징이 감지된 영역을 유지합니다. 이를 통해 특징의 위치와 상관없이 중요한 특징을 보존할 수 있습니다.\n",
    "\n",
    "4. **풀링 레이어의 파라미터 없음**: Max 풀링 및 다른 풀링 방법은 학습할 파라미터가 없으므로, 그래디언트 디센트로 학습되지 않습니다. 하이퍼파라미터(f와 s)를 설정하면 고정된 계산만 수행됩니다.\n",
    "\n",
    "5. **다른 하이퍼파라미터 예제**: 다른 하이퍼파라미터 조합에 대한 예시로 5x5 입력에 3x3 필터 크기와 스트라이드 1을 사용한 경우를 보여줍니다. 결과적으로 3x3 출력을 얻게 됩니다.\n",
    "\n",
    "6. **3D 입력에 대한 풀링**: 3D 입력(다수의 채널)에 대한 풀링은 각 채널에 대해 독립적으로 수행되며 출력 크기는 동일한 채널 수를 유지합니다.\n",
    "\n",
    "7. **평균 풀링(Average Pooling)**: Max 풀링 외에도 평균 풀링을 언급합니다. 평균 풀링은 주어진 영역의 평균을 계산하여 출력을 생성합니다. 그러나 Max 풀링이 더 자주 사용됩니다.\n",
    "\n",
    "8. **풀링의 하이퍼파라미터**: 풀링 레이어의 하이퍼파라미터로는 필터 크기(f), 스트라이드(s), 패딩(p) 등이 있으며, 이러한 하이퍼파라미터를 설정하면 고정된 계산이 이루어집니다.\n",
    "\n",
    "9. **풀링 레이어 파라미터 없음**: 풀링 레이어는 학습할 파라미터가 없으므로 고정된 함수로 작동합니다.\n",
    "\n",
    "10. **풀링 종류 선택**: 일반적으로 Max 풀링이 평균 풀링보다 더 자주 사용되지만, 특정 상황에서는 평균 풀링을 사용할 수 있습니다.\n",
    "\n",
    "풀링 레이어는 ConvNet 아키텍처에서 중요한 역할을 하며, 특징 맵의 크기를 줄이는 데 사용됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7dcc02",
   "metadata": {},
   "source": [
    "### CNN Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82dc697",
   "metadata": {},
   "source": [
    "이 비디오에서는 이미지 인식을 위한 컨볼루션 신경망(ConvNet)을 구축하는 예시를 살펴보고 있습니다. 이 아키텍처는 Yann LeCun이 개발한 고전적인 신경망인 LeNet-5에서 영감을 얻은 것입니다.\n",
    "\n",
    "아래는 이 아키텍처에 대한 개요입니다:\n",
    "\n",
    "1. 입력: 입력 이미지는 32x32x3이며, RGB 이미지를 나타냅니다.\n",
    "\n",
    "2. 레이어 1 (컨볼루션 레이어 1 - Conv1):\n",
    "   - 필터 크기: 5x5\n",
    "   - 스트라이드: 1\n",
    "   - 필터 개수: 6\n",
    "   - 출력 크기: 28x28x6\n",
    "\n",
    "3. 레이어 2 (풀링 레이어 1 - Pool1):\n",
    "   - 2x2 필터와 2의 스트라이드로 맥스 풀링을 수행합니다.\n",
    "   - 출력 크기: 14x14x6\n",
    "\n",
    "4. 레이어 3 (컨볼루션 레이어 2 - Conv2):\n",
    "   - 필터 크기: 5x5\n",
    "   - 스트라이드: 1\n",
    "   - 필터 개수: 10\n",
    "   - 출력 크기: 10x10x10\n",
    "\n",
    "5. 레이어 4 (풀링 레이어 2 - Pool2):\n",
    "   - 2x2 필터와 2의 스트라이드로 맥스 풀링을 수행합니다.\n",
    "   - 출력 크기: 5x5x10\n",
    "\n",
    "6. 레이어 5 (컨볼루션 레이어 3 - Conv3):\n",
    "   - 필터 크기: 5x5\n",
    "   - 스트라이드: 1\n",
    "   - 필터 개수: 16\n",
    "   - 출력 크기: 1x1x16 (패딩 없음)\n",
    "\n",
    "7. 레이어 6 (풀링 레이어 3 - Pool3):\n",
    "   - 2x2 필터와 2의 스트라이드로 맥스 풀링을 수행합니다.\n",
    "   - 출력 크기: 1x1x16\n",
    "\n",
    "8. 레이어 7 (완전 연결 레이어 1 - FC3):\n",
    "   - Pool2의 출력을 400x1 벡터로 변환합니다.\n",
    "   - 유닛 개수: 120\n",
    "\n",
    "9. 레이어 8 (완전 연결 레이어 2 - FC4):\n",
    "   - 유닛 개수: 84\n",
    "\n",
    "10. 레이어 9 (출력 레이어):\n",
    "    - 소프트맥스 레이어로, 숫자 인식을 위한 10개의 출력(0부터 9)을 생성합니다.\n",
    "\n",
    "주목해야 할 몇 가지 중요한 점:\n",
    "- 컨볼루션 레이어 (Conv1, Conv2, Conv3)는 전체 파라미터 대비 상대적으로 적은 파라미터를 가지고 있습니다.\n",
    "- 활성화 크기는 일반적으로 신경망의 깊이가 깊어짐에 따라 서서히 감소합니다.\n",
    "- 맥스 풀링 레이어는 파라미터가 없으며 입력을 다운샘플링합니다.\n",
    "- 완전 연결 레이어는 기존의 전통적인 신경망과 유사합니다.\n",
    "\n",
    "비디오에서 제공된 아키텍처 세부 정보는 시작점으로 작용하며, 문제와 데이터셋에 따라 특정 하이퍼파라미터를 조정할 수 있습니다. 또한, 강사는 ConvNet을 설계할 때 문학과 모범 사례를 찾을 때 실현된 것과 같이 특정 하이퍼파라미터를 선택하는 것이 중요하다고 강조하고 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b61bfe7",
   "metadata": {},
   "source": [
    "### Why Convolutions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346954a2",
   "metadata": {},
   "source": [
    "컨볼루션 레이어가 완전 연결 레이어(Fully Connected Layer) 대비하여 두 가지 주요 이점이 있는데, 이는 \"파라미터 공유(Parameter Sharing)\"와 \"연결의 희소성(Sparsity of Connections)\"입니다.\n",
    "\n",
    "1. 파라미터 공유(Parameter Sharing):\n",
    "   - 예를 들어, 32x32x3 차원 이미지를 가정해보겠습니다. 이 이미지에 5x5 필터를 6개 사용하면 28x28x6 차원의 출력이 생성됩니다.\n",
    "   - 완전 연결 레이어의 경우, 입력과 출력 간의 많은 연결이 필요합니다. 위 예시에서는 3,072개의 입력 유닛과 4,704개의 출력 유닛이 있을 경우 약 14백만 개의 파라미터가 필요합니다.\n",
    "   - 하지만 컨볼루션 레이어에서는 필터를 통해 많은 연결을 공유합니다. 각 필터는 25개의 파라미터와 1개의 바이어스 파라미터로 이루어져 있으며, 6개의 필터가 사용될 경우 총 156개의 파라미터만 필요합니다.\n",
    "   - 파라미터 공유를 통해 신경망의 파라미터 수를 줄일 수 있으며, 이는 훈련 데이터의 양을 줄이고 과적합(Overfitting)을 방지하는 데 도움이 됩니다.\n",
    "\n",
    "2. 연결의 희소성(Sparsity of Connections):\n",
    "   - 컨볼루션 레이어에서 각 출력은 작은 지역 입력 영역에만 의존합니다. 예를 들어, 3x3 필터를 사용하면 해당 필터의 3x3 입력 영역에만 연결됩니다.\n",
    "   - 이로 인해 컨볼루션 레이어의 각 출력은 입력 중 일부 특성만을 활용하며, 나머지 입력에 영향을 미치지 않습니다. 이것이 연결의 희소성입니다.\n",
    "   - 연결의 희소성을 통해 각 출력은 주변의 작은 지역 패턴에만 반응하고, 전체 입력에 대한 무작위한 연결이 없어져 연산 효율성을 높입니다.\n",
    "\n",
    "또한, 컨볼루션 신경망은 평행 이동 불변성(Translation Invariance)을 잘 캡처하는데 도움을 줍니다. 이미지가 약간 이동하더라도 동일한 특성을 캡처할 수 있도록 합니다. 이것은 이미지 인식에서 중요한 특성 중 하나입니다.\n",
    "\n",
    "마지막으로, 컨볼루션 신경망을 훈련하는 방법에 대해 간략히 언급하겠습니다. 이미지 분류 작업을 위한 레이블이 지정된 훈련 데이터셋이 주어진다고 가정해 봅시다. 이 데이터셋을 사용하여 네트워크의 매개변수를 최적화하여 비용 함수를 최소화하면 됩니다. 최적화 알고리즘으로는 경사 하강법(GD), 모멘텀(Momentum), RMSProp, Adam 등이 사용됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd97f92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

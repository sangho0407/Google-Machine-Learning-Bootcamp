{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55bb6a3b",
   "metadata": {},
   "source": [
    "# Case Studies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a2405b",
   "metadata": {},
   "source": [
    "왜 사례 연구를 살펴보는 것일까요? 최근 몇 년 동안의 컴퓨터 비전 연구는 구성 요소를 어떻게 조합하여 효과적인 합성곱 신경망을 구성할 것인지에 중점을 두었습니다. 사용자가 직관을 얻고 자신감을 갖기 위한 좋은 방법 중 하나는 이러한 사례를 보는 것입니다. 다른 사람의 코드를 읽어서 코드를 작성하는 방법을 배웠을 것으로 생각합니다. 또한 효과적인 신경망 아키텍처를 이해하고 자신의 작업에 적용할 수 있는 방법 중 하나는 다른 효과적인 사례를 보는 것입니다. 한 가지 주목할 점은 하나의 컴퓨터 비전 작업에서 잘 작동하는 신경망 아키텍처가 다른 작업에서도 잘 작동하는 경우가 많다는 것입니다. 예를 들어, 다른 사람이 고양이, 개 및 사람을 인식하는 데 아주 효과적인 신경망 아키텍처를 훈련했지만 자율 주행 자동차를 만드는 작업을 하고 있다면, 이러한 신경망 아키텍처를 가져와서 자신의 문제에 적용할 수 있을 것입니다. 마지막으로 다음 몇 개의 비디오를 통해 컴퓨터 비전 분야의 연구 논문 중 일부를 읽을 수 있게 될 것이며, 이를 이해할 수 있다면 만족스러울 것으로 기대합니다. 이것을 수업처럼 하지 않아도 만족스러울 수 있으며, 컴퓨터 비전 연구 논문 중 몇 개를 읽고 이해할 수 있다면 기쁠 것입니다. 그럼 이제 시작해 봅시다. 다음 몇 개의 비디오에서는 몇 가지 고전적인 네트워크를 보여줄 것입니다. LeNet-5 네트워크는 1980년대에 나온 것으로, AlexNet과 VGG 네트워크가 자주 언급됩니다. 이러한 논문에서 나온 아이디어는 여러분의 작업에 유용할 것으로 예상됩니다. 그런 다음 ResNet 또는 잔차 네트워크를 보여 드리겠습니다. 신경망이 점점 깊어지고 있다고 들어본 적이 있을 것입니다. ResNet 신경망은 매우 깊은 152층 신경망을 훈련시킨 것으로, 이를 효과적으로 수행하는 데 흥미로운 트릭과 아이디어를 가지고 있습니다. 마지막으로 Inception 신경망의 사례 연구도 살펴보겠습니다. 이러한 신경망을 보고 나면 효과적인 합성곱 신경망을 어떻게 구축해야 하는지에 대한 직관이 훨씬 더 높아질 것으로 생각합니다. 컴퓨터 비전 작업을 하지 않더라도 이러한 아이디어 중 많은 것들이 다른 분야로 확산되고 있으므로, 컴퓨터 비전 응용프로그램을 만들지 않더라도 이러한 아이디어 중 일부가 굉장히 흥미롭고 도움이 될 것으로 생각합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193029e4",
   "metadata": {},
   "source": [
    "### Classic Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad5770d",
   "metadata": {},
   "source": [
    "**LeNet-5**:\n",
    "- 입력 이미지: 32x32x1 (흑백 이미지)\n",
    "- 목적: 손글씨 숫자 인식\n",
    "- 구조: \n",
    "  - 합성곱 레이어 (6개의 5x5 필터, 스트라이드 1)\n",
    "  - 평균 풀링 레이어 (2x2 필터, 스트라이드 2)\n",
    "  - 추가 합성곱 레이어 (16개의 5x5 필터, 스트라이드 1)\n",
    "  - 또 다른 평균 풀링 레이어\n",
    "  - 완전 연결 레이어 (400개의 노드, 120개의 뉴런)\n",
    "  - 추가 완전 연결 레이어\n",
    "  - 최종 출력 (10가지 클래스 분류)\n",
    "\n",
    "**AlexNet**:\n",
    "- 입력 이미지: 227x227x3 (컬러 이미지)\n",
    "- 목적: 컴퓨터 비전 분야의 딥 러닝 도입\n",
    "- 구조: \n",
    "  - 합성곱 레이어 (96개의 11x11 필터, 스트라이드 4)\n",
    "  - 맥스 풀링 레이어 (3x3 필터, 스트라이드 2)\n",
    "  - 추가 합성곱 레이어 및 맥스 풀링 레이어 반복\n",
    "  - 완전 연결 레이어 (4,096개의 노드)\n",
    "  - softmax 출력 레이어 (1,000개의 클래스 분류)\n",
    "\n",
    "**VGG-16**:\n",
    "- 입력 이미지: 224x224x3 (컬러 이미지)\n",
    "- 목적: 간단하면서 효과적인 딥 러닝 아키텍처 제시\n",
    "- 구조: \n",
    "  - 합성곱 레이어 (3x3 필터, 스트라이드 1, same 패딩)\n",
    "  - 맥스 풀링 레이어 (2x2 필터, 스트라이드 2)\n",
    "  - 다양한 합성곱 레이어와 맥스 풀링 레이어 반복\n",
    "  - 완전 연결 레이어 (4,096개의 노드)\n",
    "  - softmax 출력 레이어 (1,000개의 클래스 분류)\n",
    "\n",
    "이러한 고전적인 신경망 아키텍처는 딥러닝의 발전 과정에서 중요한 역할을 하였으며, 각각의 특징과 개선 사항을 이해하면 현대 신경망 설계에도 도움이 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61654ad4",
   "metadata": {},
   "source": [
    "### ResNets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a809e849",
   "metadata": {},
   "source": [
    "Residual Networks (ResNets)는 매우 깊은 신경망을 효과적으로 훈련시키기 위한 아키텍처\n",
    "\n",
    "- **신경망의 어려움:** 매우 깊은 신경망은 사라지거나 폭발하는 그래디언트(gradient) 문제 때문에 훈련하기 어려운데, ResNets는 이러한 어려움을 극복하는데 도움을 줍니다.\n",
    "\n",
    "- **스킵 연결(skip connections):** ResNets의 핵심 개념은 스킵 연결입니다. 이것은 한 레이어의 활성화 값을 다른 레이어로 전달하는 것을 가능하게 합니다. 이렇게 하면 매우 깊은 네트워크를 구축할 수 있습니다.\n",
    "\n",
    "- **잔여 블록(residual block):** ResNets는 잔여 블록으로 구성됩니다. 잔여 블록은 레이어 간의 정보 흐름을 개선하기 위한 개념입니다.\n",
    "\n",
    "- **잔여 블록의 동작:** 잔여 블록은 두 레이어로 구성되어 있으며, 첫 번째 레이어에서는 입력 a[l]에 선형 연산을 적용한 후 ReLU 활성화 함수를 적용하여 a[l+1]을 얻습니다. 그런 다음 두 번째 레이어에서도 비슷한 과정을 거칩니다. 이로써 정보는 주로 경로(main path)를 따라 흐릅니다.\n",
    "\n",
    "- **스킵 연결 추가:** ResNets에서는 정보가 주로 흐르는 경로에 스킵 연결을 추가합니다. 이것은 a[l]을 복사하고, ReLU 비선형성을 적용하기 전에 신경망의 더 깊은 부분으로 전달합니다. 이를 통해 정보는 주요 경로를 따르지 않고도 더 깊은 신경망으로 쉽게 이동할 수 있습니다.\n",
    "\n",
    "- **성능 향상:** ResNets의 개발자들은 잔여 블록을 사용하여 매우 깊은 신경망을 효과적으로 훈련할 수 있다는 것을 발견했습니다. 이를 통해 ResNet은 100개 이상의 레이어로 이루어진 네트워크도 훈련할 수 있으며, 일부는 1000개 이상의 레이어를 사용한 실험도 진행되었습니다.\n",
    "\n",
    "- **성능 안정성:** ResNets를 사용하면 그래디언트가 사라지거나 폭발하는 문제를 완화시키고, 네트워크를 더 깊게 훈련할 수 있습니다. 이로 인해 훈련 에러가 깊은 네트워크에서도 계속 감소할 수 있으며, 최종적으로 높은 성능을 유지하거나 개선할 수 있습니다.\n",
    "\n",
    "- **구현과 활용:** ResNets는 매우 깊은 신경망을 효과적으로 훈련시키기 위한 강력한 도구로, 실제로 사용되는 사례들을 구현하여 자세히 살펴보는 프로그래밍 연습도 가능합니다.\n",
    "\n",
    "**Residual Networks (ResNets)이 잘 동작하는 이유:**\n",
    "\n",
    "ResNets는 매우 깊은 신경망을 효과적으로 훈련시킬 수 있습니다. 이러한 성능 향상의 핵심 원리를 설명하기 위해 다음과 같은 예시를 살펴봅니다:\n",
    "\n",
    "- 네트워크를 깊게 만드는 것은 종종 훈련 성능을 저하시킬 수 있습니다. 그러나 ResNets에서는 깊은 네트워크를 훈련하는 데 더 이상한 점이 없습니다.\n",
    "\n",
    "- 네트워크의 레이어를 추가하는 경우, ResNet 블록을 도입하여 추가한 레이어에서 입력(a[l])을 출력(a[l+2])까지 쉽게 복사할 수 있도록 합니다.\n",
    "\n",
    "- ResNet 블록은 항등 함수(identity mapping)를 학습하기에 용이하며, 이 항등 함수는 입력(a[l])을 출력(a[l+2])으로 복사하는 것을 의미합니다.\n",
    "\n",
    "- 활성화 함수로 ReLU를 사용하는 경우, 모든 활성화는 음수가 아닌 값을 가집니다. 따라서 ResNet 블록에서 추가한 레이어들은 입력을 복사하는 것을 배우는 데 큰 어려움이 없습니다.\n",
    "\n",
    "- 이로 인해 네트워크에 레이어를 추가하는 것이 성능에 큰 영향을 미치지 않습니다. 더불어, 합리적인 베이스라인에서 시작하므로 성능을 개선할 여지가 있습니다.\n",
    "\n",
    "- ResNets의 핵심 아이디어는 항등 함수를 쉽게 학습할 수 있게 하여 네트워크를 더 깊게 만들더라도 성능 저하를 최소화하고 심지어 높은 성능을 달성할 수 있게 합니다.\n",
    "\n",
    "ResNets는 네트워크를 깊게 만들어도 훈련 가능하게 하며, 추가된 레이어가 항등 함수를 학습하면서 높은 성능을 유지할 수 있도록 도와줍니다. 이는 매우 깊은 신경망을 사용하여 높은 정확도를 달성하는 데 중요한 기술 중 하나입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93db25d6",
   "metadata": {},
   "source": [
    "### Networks in Networks and 1x1 Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feff1d70",
   "metadata": {},
   "source": [
    "\n",
    "- **원 바이 원 합성곱이란:** 원 바이 원 컨볼루션은 커널 크기가 1x1인 컨볼루션 연산을 의미합니다. 이것은 단순히 숫자를 곱하는 것처럼 보일 수 있지만, 실제로는 다양한 작업을 수행할 수 있습니다.\n",
    "\n",
    "- **원 바이 원 합성곱의 동작:** 1x1 컨볼루션을 사용하면 입력 데이터와 커널 간의 요소별 곱셈을 수행하고, 그 결과에 ReLU 활성화 함수를 적용합니다. 이것은 여러 위치에서 동시에 이루어집니다.\n",
    "\n",
    "- **다중 필터 사용:** 여러 개의 1x1 컨볼루션 필터를 사용할 수 있으며, 각 필터는 입력 데이터와 요소별 곱셈을 수행하고 ReLU를 적용합니다. 이를 통해 여러 개의 출력을 생성할 수 있습니다.\n",
    "\n",
    "- **네트워크 내 네트워크(NiN):** 이 아이디어는 종종 \"네트워크 내 네트워크(NiN)\"라고 불리며, 입력의 각 위치에 대해 다양한 작업을 수행하는 일종의 작은 신경망을 적용하는 것으로 생각할 수 있습니다.\n",
    "\n",
    "- **채널 수 조절:** 1x1 컨볼루션을 사용하여 채널 수를 조절할 수 있으며, 이를 통해 계산 리소스를 절약하거나 더 복잡한 네트워크를 만들 수 있습니다.\n",
    "\n",
    "- **예시:** 예를 들어, 28x28x192 크기의 입력 데이터가 있다고 가정해 봅시다. 채널 수를 줄이려면 1x1 컨볼루션을 사용하여 32개의 출력 채널을 생성할 수 있습니다. 이를 통해 채널 수를 조절하거나 유지하거나 증가시킬 수 있습니다.\n",
    "\n",
    "- **복잡한 함수 학습:** 원 바이 원 컨볼루션은 네트워크에 더 복잡한 함수를 학습할 수 있는 또 다른 레이어를 추가하는 방식으로 사용될 수 있습니다.\n",
    "\n",
    "원 바이 원 컨볼루션은 채널 수를 조절하고 네트워크의 복잡성을 조절하는 데 사용될 수 있으며, 이것은 인셉션 네트워크(Inception Network)와 같은 다른 네트워크 아키텍처의 기초로 활용됩니다. 이것은 신경망의 효율적인 설계와 리소스 관리에 중요한 역할을 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c416f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
